{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "import torch as th\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from torch import optim\n",
    "from src.model.replayBuffer import ReplayBuffer\n",
    "# from simulation import *\n",
    "from src.simulationUtils.simulation import *\n",
    "from src.simulationUtils import *\n",
    "\n",
    "device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(10,100)\n",
    "        self.fc5 = nn.Linear(100,5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc5(x)\n",
    "        return x\n",
    "\n",
    "    def sample_action(self, obs, epsilon):\n",
    "        out = self.forward(obs)\n",
    "        coin = random.random()\n",
    "        if coin < epsilon:\n",
    "            return random.randint(0, 4)\n",
    "        else:\n",
    "            return out.argmax().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(q, q_target, replay_buffer, optimizer, batch_size, gamma, updates_number=10):\n",
    "    for _ in range(updates_number):\n",
    "\n",
    "        s, a, r, s_prime, done_mask = replay_buffer.sample(batch_size)\n",
    "\n",
    "        # полезность\n",
    "        q_out = q(s)\n",
    "        a = a.unsqueeze(1)\n",
    "        q_a = q_out.gather(1, a)\n",
    "        max_q_prime = q_target(s_prime).max(1)[0].unsqueeze(1)\n",
    "        target = r.unsqueeze(1) +  gamma * max_q_prime * done_mask.unsqueeze(1)\n",
    "\n",
    "        loss = F.smooth_l1_loss(q_a, target.detach())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = list()\n",
    "total = list()\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "# DEFAULT_STATE = (0,0,0,0,0,0,0,0,0,0)\n",
    "\n",
    "def run(learning_rate, gamma, buffer_max_size, batch_size, target_update_interval,\n",
    "        replay_buffer_start_size, print_interval=20, n_episodes=10000):\n",
    "\n",
    "    q = QNetwork()\n",
    "    q_target = QNetwork()\n",
    "\n",
    "    q_target.load_state_dict(q.state_dict())\n",
    "\n",
    "    replay_buffer = ReplayBuffer(max_size=buffer_max_size)\n",
    "\n",
    "    score = 0.0\n",
    "\n",
    "    optimizer = optim.Adam(q.parameters(), lr=learning_rate)\n",
    "\n",
    "    for n_epi in range(n_episodes):\n",
    "        epsilon = max(0.01, 0.08 - 0.01 * (n_epi / 200))\n",
    "        env = initSimulation()\n",
    "        s, _, _ = env.step({0:0})\n",
    "        s = np.array(s)\n",
    "        s = s.reshape(-1)\n",
    "        \n",
    "        for g in range(300):\n",
    "            s = np.array(s)\n",
    "            a = q.sample_action(th.from_numpy(s).float(), epsilon)\n",
    "            # s_prime, r,  terminated, truncated, _ = env.step(a)\n",
    "            s_prime, r, _ = env.step({0:a})\n",
    "\n",
    "            done_mask = 0.0  if (tuple(s_prime) == DEFAULT_STATE or tuple(s) == DEFAULT_STATE) else 1.0\n",
    "            replay_buffer.put((s, a, r/100.0, s_prime, done_mask))\n",
    "            s = s_prime\n",
    "            score += r\n",
    "            if g % 10 == 0 and len(replay_buffer) > replay_buffer_start_size:\n",
    "                train(q, q_target, replay_buffer, optimizer, batch_size, gamma)\n",
    "\n",
    "\n",
    "            # if truncated or terminated:\n",
    "            #     break\n",
    "        if n_epi % 20:\n",
    "            print(step)\n",
    "            print(total)\n",
    "        # writer.add_scalar('reward/train', score , n_epi)\n",
    "        env.close()\n",
    "        step.append(n_epi)\n",
    "        total.append(score)\n",
    "        print(len(replay_buffer))\n",
    "        # if len(replay_buffer) > replay_buffer_start_size:\n",
    "        #     train(q, q_target, replay_buffer, optimizer, batch_size, gamma)\n",
    "\n",
    "\n",
    "        if n_epi % target_update_interval == 0 and n_epi != 0:\n",
    "            q_target.load_state_dict(q.state_dict())\n",
    "        print(\"# of episode :{}, abg score : {:.1f}, buffer size : {}, epsilon : {:.1f}%\"\n",
    "                .format(n_epi, score/ print_interval, len(replay_buffer), epsilon * 100))\n",
    "        score = 0.0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run(learning_rate=0.001,\n",
    "#     gamma=0.98,\n",
    "#     buffer_max_size=10000,\n",
    "#     batch_size=64,\n",
    "#     target_update_interval=10,\n",
    "#     replay_buffer_start_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.simulationUtils.simulation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.simulationUtils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.simulationUtils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pettingzoo.butterfly import knights_archers_zombies_v10\n",
    "# env = knights_archers_zombies_v10.env(render_mode=\"human\")\n",
    "# env.reset(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = initSimulation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.from_id_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(100):\n",
    "#     env.step({0:0,1:0,2:0,3:0,4:0,5:0})\n",
    "#     print(env.use_real_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.step(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(1000):\n",
    "#     v = env.step(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Мультиагентное обучение с подкреплением\n",
    "#Глава 3. Нейросетевое обучение\n",
    "#Алгоритм MADDPG\n",
    "\n",
    "#Подключаем библиотеки\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "#Флаг вывода массива целиком\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "\n",
    "#Определяем архитектуру нейронной сети исполнителя\n",
    "class MADDPG_Actor(nn.Module):\n",
    "    def __init__(self, obs_size, n_actions):\n",
    "        super(MADDPG_Actor, self).__init__()\n",
    "        #На вход нейронная сеть получает состояние среды для отдельного агента \n",
    "        #На выходе нейронная сеть возвращает стратегию действий\n",
    "        self.MADDPG_Actor = nn.Sequential(\n",
    "            #Первый линейный слой обрабатывает входные данные состояния среды\n",
    "            nn.Linear(obs_size, 60),\n",
    "            nn.ReLU(),\n",
    "            #Второй линейный слой обрабатывает внутренние данные \n",
    "            nn.Linear(60, 60),\n",
    "            nn.ReLU(),\n",
    "            #Третий линейный слой обрабатывает внутренние данные \n",
    "            nn.Linear(60, 60),\n",
    "            nn.ReLU(),\n",
    "            #Четвертый линейный слой обрабатывает данные для стратегии действий\n",
    "            nn.Linear(60, n_actions)\n",
    "            )\n",
    "        #Финальный выход нерйонной сети обрабатывается функцией Tanh()\n",
    "        self.tanh_layer = nn.Tanh()\n",
    "    #Вначале данные x обрабатываются полносвязной сетью с функцией ReLU\n",
    "    #На выходе происходит обработка функцией Tanh()\n",
    "    def forward(self, x):\n",
    "        #Обработка полносвязными линейными слоями\n",
    "        network_out = self.MADDPG_Actor(x)\n",
    "        #Обработка функцией Tanh()\n",
    "        tanh_layer_out = self.tanh_layer(network_out)\n",
    "        #Выход нейронной сети\n",
    "        return tanh_layer_out\n",
    "\n",
    "#Определяем архитектуру нейронной сети критика\n",
    "class MADDPG_Critic(nn.Module):\n",
    "    def __init__(self, full_obs_size, n_actions_agents):\n",
    "        super(MADDPG_Critic, self).__init__()\n",
    "        #На вход нейронная сеть получает состояние среды,\n",
    "        #включающее все локальные состояния среды от отдельных агентов\n",
    "        #и все выполненные действия отдельных агентов\n",
    "        #На выходе нейронная сеть возвращает корректирующее значение\n",
    "        self.network = nn.Sequential(\n",
    "            #Первый линейный слой обрабатывает входные данные    \n",
    "            nn.Linear(full_obs_size+n_actions_agents, 202),\n",
    "            nn.ReLU(),\n",
    "            #Второй линейный слой обрабатывает внутренние данные\n",
    "            nn.Linear(202, 60),\n",
    "            nn.ReLU(),\n",
    "            #Третий линейный слой обрабатывает внутренние данные\n",
    "            nn.Linear(60, 30),\n",
    "            nn.ReLU(),\n",
    "            #Четвертый линейный слой обрабатывает выходные данные\n",
    "            nn.Linear(30, 1)\n",
    "            )\n",
    "    #Данные x последовательно обрабатываются полносвязной сетью с функцией ReLU\n",
    "    def forward(self, state, action):\n",
    "        #Объединяем данные состояний и действий для передачи в сеть\n",
    "        x = torch.cat([state, action], dim=2)\n",
    "        #Результаты обработки \n",
    "        Q_value = self.network(x)\n",
    "        #Финальный выход нейронной сети\n",
    "        return Q_value\n",
    "    \n",
    "\n",
    "        \n",
    "#Выбираем возможное действие с максимальным из стратегии действий\n",
    "#с учетом дополнительного случайного шума\n",
    "def select_actionFox(act_prob, avail_actions_ind, n_actions, noise_rate):\n",
    "    p = np.random.random(1).squeeze()\n",
    "    #Добавляем случайный шум к действиям для исследования\n",
    "    #разных вариантов действий\n",
    "    for i in range(n_actions):\n",
    "        #Создаем шум заданного уровня\n",
    "        noise = noise_rate*(np.random.rand())\n",
    "        #Добавляем значение шума к значению вероятности выполнения действия\n",
    "        act_prob [i] =  act_prob [i] + noise\n",
    "    #Выбираем действия в зависимости от вероятностей их выполнения\n",
    "    for j in range(n_actions):\n",
    "        #Выбираем случайный элемент из списка \n",
    "        actiontemp =  random.choices([str(i) for i in range(n_actions)], weights=softmax(act_prob))\n",
    "        #Преобразуем тип данных\n",
    "        action = int (actiontemp[0])\n",
    "        #Проверяем наличие выбранного действия в списке действий\n",
    "        if action in avail_actions_ind:\n",
    "            return action\n",
    "        else:\n",
    "            act_prob[action] = 0\n",
    "          \n",
    "#Создаем минивыборку определенного объема из буфера воспроизведения        \n",
    "def sample_from_expbuf(experience_buffer, batch_size):\n",
    "    #Функция возвращает случайную последовательность заданной длины\n",
    "    perm_batch = np.random.permutation(len(experience_buffer))[:batch_size]\n",
    "    #Минивыборка\n",
    "    experience = np.array(experience_buffer)[perm_batch]\n",
    "    #Возвращаем значения минивыборки по частям\n",
    "    return experience[:,0], experience[:,1], experience[:,2], experience[:,3], experience[:,4], experience[:,5]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFAULT_STATE = (0,0,0,0,0,0,0,0,0,0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Retrying in 1 seconds\n",
      "obs_size= 18\n",
      "Actor_network_list= [MADDPG_Actor(\n",
      "  (MADDPG_Actor): Sequential(\n",
      "    (0): Linear(in_features=18, out_features=60, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=60, out_features=60, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=60, out_features=60, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=60, out_features=6, bias=True)\n",
      "  )\n",
      "  (tanh_layer): Tanh()\n",
      "), MADDPG_Actor(\n",
      "  (MADDPG_Actor): Sequential(\n",
      "    (0): Linear(in_features=18, out_features=60, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=60, out_features=60, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=60, out_features=60, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=60, out_features=6, bias=True)\n",
      "  )\n",
      "  (tanh_layer): Tanh()\n",
      "), MADDPG_Actor(\n",
      "  (MADDPG_Actor): Sequential(\n",
      "    (0): Linear(in_features=18, out_features=60, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=60, out_features=60, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=60, out_features=60, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=60, out_features=6, bias=True)\n",
      "  )\n",
      "  (tanh_layer): Tanh()\n",
      ")]\n",
      "Critic_network_list= MADDPG_Critic(\n",
      "  (network): Sequential(\n",
      "    (0): Linear(in_features=57, out_features=202, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=202, out_features=60, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=60, out_features=30, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=30, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "---> [Mean Time for Edge E1:\n",
      "ordinary   ---        nan\n",
      "connected  ---        nan\n",
      "All        ---        nan\n",
      ", Mean Fuel Consumption for Edge E1:\n",
      "ordinary   ---        nan\n",
      "connected  ---        nan\n",
      "All        ---        nan\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ilya_zolotukhin/.local/lib/python3.11/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/ilya_zolotukhin/.local/lib/python3.11/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Retrying in 1 seconds\n",
      "noise_rate= 0.9\n",
      "global_step= 501 Total reward in episode 0 = -692972.6715848782\n",
      "loss1 []\n",
      "loss2 []\n",
      "---> [Mean Time for Edge E1:\n",
      "ordinary   ---    55.2597\n",
      "connected  ---    56.8485\n",
      "All        ---    55.7364\n",
      ", Mean Fuel Consumption for Edge E1:\n",
      "ordinary   --- 95586.8372\n",
      "connected  --- 110731.6876\n",
      "All        --- 100130.2923\n",
      "]\n",
      " Retrying in 1 seconds\n",
      "noise_rate= 0.870274\n",
      "global_step= 1002 Total reward in episode 1 = -548594.8632856769\n",
      "loss1 []\n",
      "loss2 []\n",
      "---> [Mean Time for Edge E1:\n",
      "ordinary   ---    55.2500\n",
      "connected  ---    58.3111\n",
      "All        ---    56.3520\n",
      ", Mean Fuel Consumption for Edge E1:\n",
      "ordinary   --- 91928.2356\n",
      "connected  --- 100276.5062\n",
      "All        --- 94933.6130\n",
      "]\n",
      " Retrying in 1 seconds\n",
      "noise_rate= 0.8405480000000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_51560/3053140413.py:237: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:274.)\n",
      "  obs_agentsT = torch.FloatTensor([exp_obs]).to(device)\n",
      "/home/ilya_zolotukhin/.local/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at ../aten/src/ATen/cuda/CublasHandlePool.cpp:135.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_step= 1503 Total reward in episode 2 = -597414.278659103\n",
      "loss1 [array(19753784., dtype=float32), array(59150968., dtype=float32), array(42566192., dtype=float32), array(38990500., dtype=float32), array(6927220., dtype=float32), array(90262216., dtype=float32), array(41429480., dtype=float32), array(52987156., dtype=float32), array(65120000., dtype=float32), array(39201280., dtype=float32)]\n",
      "loss2 [array(29564.48, dtype=float32), array(28433.043, dtype=float32), array(30323.662, dtype=float32), array(29259.023, dtype=float32), array(37348.44, dtype=float32), array(36547.766, dtype=float32), array(34561.75, dtype=float32), array(39526.992, dtype=float32), array(39286.867, dtype=float32), array(43166.97, dtype=float32)]\n",
      "---> [Mean Time for Edge E1:\n",
      "ordinary   ---    56.8571\n",
      "connected  ---    58.1290\n",
      "All        ---    57.1803\n",
      ", Mean Fuel Consumption for Edge E1:\n",
      "ordinary   --- 95730.5297\n",
      "connected  --- 102217.7459\n",
      "All        --- 97378.9207\n",
      "]\n",
      " Retrying in 1 seconds\n",
      "noise_rate= 0.810822\n",
      "global_step= 2004 Total reward in episode 3 = -621140.3714988802\n",
      "loss1 [array(1.2648334e+08, dtype=float32), array(29926080., dtype=float32), array(82730800., dtype=float32), array(66616896., dtype=float32), array(1.047355e+08, dtype=float32), array(1.101949e+08, dtype=float32), array(53413840., dtype=float32), array(23089846., dtype=float32), array(51703408., dtype=float32), array(65206508., dtype=float32)]\n",
      "loss2 [array(59983.855, dtype=float32), array(62775.87, dtype=float32), array(56133.133, dtype=float32), array(45510.42, dtype=float32), array(49431.492, dtype=float32), array(47360.008, dtype=float32), array(37850.5, dtype=float32), array(45854.504, dtype=float32), array(41160.383, dtype=float32), array(27239.664, dtype=float32)]\n",
      "---> [Mean Time for Edge E1:\n",
      "ordinary   ---    56.3053\n",
      "connected  ---    57.7500\n",
      "All        ---    56.6693\n",
      ", Mean Fuel Consumption for Edge E1:\n",
      "ordinary   --- 95102.9068\n",
      "connected  --- 107291.1757\n",
      "All        --- 98173.9667\n",
      "]\n",
      " Retrying in 1 seconds\n",
      "noise_rate= 0.781096\n",
      "global_step= 2505 Total reward in episode 4 = -709262.9082151472\n",
      "loss1 [array(74246160., dtype=float32), array(1.7674317e+08, dtype=float32), array(54861992., dtype=float32), array(1.0667576e+08, dtype=float32), array(30368648., dtype=float32), array(43367168., dtype=float32), array(38175792., dtype=float32), array(1.2193106e+08, dtype=float32), array(27543072., dtype=float32), array(70091864., dtype=float32)]\n",
      "loss2 [array(37836.97, dtype=float32), array(37461.266, dtype=float32), array(36974.613, dtype=float32), array(44577.46, dtype=float32), array(34565.7, dtype=float32), array(31107.352, dtype=float32), array(35389.5, dtype=float32), array(28988.09, dtype=float32), array(27057.434, dtype=float32), array(29909.867, dtype=float32)]\n",
      "---> [Mean Time for Edge E1:\n",
      "ordinary   ---    56.2073\n",
      "connected  ---    55.7667\n",
      "All        ---    56.0893\n",
      ", Mean Fuel Consumption for Edge E1:\n",
      "ordinary   --- 94193.6980\n",
      "connected  --- 107788.2487\n",
      "All        --- 97835.0955\n",
      "]\n",
      " Retrying in 1 seconds\n",
      "noise_rate= 0.7513700000000001\n",
      "global_step= 3006 Total reward in episode 5 = -589153.2368440961\n",
      "loss1 [array(48191460., dtype=float32), array(38863060., dtype=float32), array(60339272., dtype=float32), array(65858040., dtype=float32), array(1.4633866e+08, dtype=float32), array(45999436., dtype=float32), array(70408568., dtype=float32), array(99861256., dtype=float32), array(92764368., dtype=float32), array(68392528., dtype=float32)]\n",
      "loss2 [array(43783.094, dtype=float32), array(38860.152, dtype=float32), array(37186.355, dtype=float32), array(48285.195, dtype=float32), array(47639.625, dtype=float32), array(47890.39, dtype=float32), array(47340.08, dtype=float32), array(52451.023, dtype=float32), array(39972.426, dtype=float32), array(38835.715, dtype=float32)]\n",
      "---> [Mean Time for Edge E1:\n",
      "ordinary   ---    55.8778\n",
      "connected  ---    59.0312\n",
      "All        ---    56.7049\n",
      ", Mean Fuel Consumption for Edge E1:\n",
      "ordinary   --- 96407.2843\n",
      "connected  --- 113599.3612\n",
      "All        --- 100916.6815\n",
      "]\n",
      " Retrying in 1 seconds\n",
      "noise_rate= 0.721644\n",
      "global_step= 3507 Total reward in episode 6 = -663310.9695228707\n",
      "loss1 [array(45957116., dtype=float32), array(19031628., dtype=float32), array(86211288., dtype=float32), array(14639840., dtype=float32), array(72832816., dtype=float32), array(2.180811e+08, dtype=float32), array(1.2373635e+08, dtype=float32), array(73842480., dtype=float32), array(1.2626804e+08, dtype=float32), array(19158692., dtype=float32)]\n",
      "loss2 [array(33436.387, dtype=float32), array(40475.496, dtype=float32), array(33742.34, dtype=float32), array(38199.266, dtype=float32), array(33814.285, dtype=float32), array(37564.29, dtype=float32), array(37294.66, dtype=float32), array(40635.145, dtype=float32), array(39242.773, dtype=float32), array(45395.14, dtype=float32)]\n",
      "---> [Mean Time for Edge E1:\n",
      "ordinary   ---    56.7674\n",
      "connected  ---    56.5500\n",
      "All        ---    56.6984\n",
      ", Mean Fuel Consumption for Edge E1:\n",
      "ordinary   --- 93744.8538\n",
      "connected  --- 99161.5235\n",
      "All        --- 95464.4315\n",
      "]\n",
      " Retrying in 1 seconds\n",
      "noise_rate= 0.691918\n",
      "global_step= 4008 Total reward in episode 7 = -657255.8885292114\n",
      "loss1 [array(2.1206211e+08, dtype=float32), array(56095056., dtype=float32), array(3.1933178e+08, dtype=float32), array(41158176., dtype=float32), array(77027856., dtype=float32), array(34435344., dtype=float32), array(33555096., dtype=float32), array(24937660., dtype=float32), array(27722718., dtype=float32), array(20772100., dtype=float32)]\n",
      "loss2 [array(44332., dtype=float32), array(43244.82, dtype=float32), array(42203.293, dtype=float32), array(42086.605, dtype=float32), array(35142.81, dtype=float32), array(33494.11, dtype=float32), array(36843.297, dtype=float32), array(38873.56, dtype=float32), array(34239.918, dtype=float32), array(35030.336, dtype=float32)]\n",
      "---> [Mean Time for Edge E1:\n",
      "ordinary   ---    54.7375\n",
      "connected  ---    56.7429\n",
      "All        ---    55.3478\n",
      ", Mean Fuel Consumption for Edge E1:\n",
      "ordinary   --- 93569.5424\n",
      "connected  --- 105511.0464\n",
      "All        --- 97203.9131\n",
      "]\n",
      " Retrying in 1 seconds\n",
      "noise_rate= 0.662192\n",
      "global_step= 4509 Total reward in episode 8 = -679255.4820341843\n",
      "loss1 [array(55321760., dtype=float32), array(9312833., dtype=float32), array(16369567., dtype=float32), array(29041294., dtype=float32), array(23002008., dtype=float32), array(49178472., dtype=float32), array(35356236., dtype=float32), array(17519956., dtype=float32), array(45423616., dtype=float32), array(23110820., dtype=float32)]\n",
      "loss2 [array(21403.041, dtype=float32), array(23314.844, dtype=float32), array(23359.994, dtype=float32), array(28881.652, dtype=float32), array(25688.332, dtype=float32), array(30331.902, dtype=float32), array(28654.27, dtype=float32), array(28431.799, dtype=float32), array(33458.445, dtype=float32), array(37933.734, dtype=float32)]\n",
      "---> [Mean Time for Edge E1:\n",
      "ordinary   ---    53.6667\n",
      "connected  ---    55.5417\n",
      "All        ---    54.1212\n",
      ", Mean Fuel Consumption for Edge E1:\n",
      "ordinary   --- 89735.6284\n",
      "connected  --- 106538.1744\n",
      "All        --- 93808.9729\n",
      "]\n",
      " Retrying in 1 seconds\n",
      "noise_rate= 0.632466\n",
      "global_step= 5010 Total reward in episode 9 = -789651.7496503965\n",
      "loss1 [array(30217162., dtype=float32), array(20988680., dtype=float32), array(40609252., dtype=float32), array(2.4457237e+08, dtype=float32), array(74641208., dtype=float32), array(37838940., dtype=float32), array(1.2736357e+08, dtype=float32), array(1.2186995e+08, dtype=float32), array(16420396., dtype=float32), array(2.029104e+08, dtype=float32)]\n",
      "loss2 [array(40396.93, dtype=float32), array(38424.492, dtype=float32), array(45647.176, dtype=float32), array(48325.242, dtype=float32), array(45808.023, dtype=float32), array(45646.89, dtype=float32), array(45412.22, dtype=float32), array(37135.453, dtype=float32), array(34892.383, dtype=float32), array(36289.43, dtype=float32)]\n",
      "---> [Mean Time for Edge E1:\n",
      "ordinary   ---    55.2099\n",
      "connected  ---    56.0909\n",
      "All        ---    55.4649\n",
      ", Mean Fuel Consumption for Edge E1:\n",
      "ordinary   --- 93997.0210\n",
      "connected  --- 109521.0382\n",
      "All        --- 98490.8154\n",
      "]\n",
      " Retrying in 1 seconds\n",
      "noise_rate= 0.60274\n",
      "global_step= 5511 Total reward in episode 10 = -715510.9598838879\n",
      "loss1 [array(39218388., dtype=float32), array(52106720., dtype=float32), array(38170740., dtype=float32), array(1.5193365e+08, dtype=float32), array(1.1844482e+08, dtype=float32), array(91531696., dtype=float32), array(11424733., dtype=float32), array(12978276., dtype=float32), array(39397580., dtype=float32), array(1.1394758e+08, dtype=float32)]\n",
      "loss2 [array(29125.422, dtype=float32), array(36044.17, dtype=float32), array(30448.695, dtype=float32), array(29601.816, dtype=float32), array(33156.383, dtype=float32), array(31491.055, dtype=float32), array(32810.758, dtype=float32), array(33177.08, dtype=float32), array(40048.047, dtype=float32), array(36595.33, dtype=float32)]\n",
      "---> [Mean Time for Edge E1:\n",
      "ordinary   ---    54.3333\n",
      "connected  ---    56.9524\n",
      "All        ---    55.3243\n",
      ", Mean Fuel Consumption for Edge E1:\n",
      "ordinary   --- 93597.7412\n",
      "connected  --- 108079.5585\n",
      "All        --- 99077.3477\n",
      "]\n",
      " Retrying in 1 seconds\n",
      "noise_rate= 0.573014\n",
      "global_step= 6012 Total reward in episode 11 = -654574.7799590749\n",
      "loss1 [array(73537000., dtype=float32), array(1.9241834e+08, dtype=float32), array(87140408., dtype=float32), array(2.4047544e+08, dtype=float32), array(3.9435443e+08, dtype=float32), array(36336312., dtype=float32), array(39959948., dtype=float32), array(50338772., dtype=float32), array(3.273109e+08, dtype=float32), array(79373832., dtype=float32)]\n",
      "loss2 [array(59406.777, dtype=float32), array(57478.473, dtype=float32), array(59752.12, dtype=float32), array(60105.5, dtype=float32), array(56699.65, dtype=float32), array(57381.477, dtype=float32), array(64376.336, dtype=float32), array(64828.477, dtype=float32), array(58400.26, dtype=float32), array(53449.207, dtype=float32)]\n",
      "---> [Mean Time for Edge E1:\n",
      "ordinary   ---    54.9726\n",
      "connected  ---    55.1951\n",
      "All        ---    55.0526\n",
      ", Mean Fuel Consumption for Edge E1:\n",
      "ordinary   --- 93010.8097\n",
      "connected  --- 102293.2362\n",
      "All        --- 96349.2262\n",
      "]\n",
      " Retrying in 1 seconds\n",
      "noise_rate= 0.543288\n",
      "global_step= 6513 Total reward in episode 12 = -681033.138509999\n",
      "loss1 [array(51800680., dtype=float32), array(66671660., dtype=float32), array(54572836., dtype=float32), array(26019150., dtype=float32), array(35253824., dtype=float32), array(37270712., dtype=float32), array(1.731692e+08, dtype=float32), array(56680692., dtype=float32), array(1.3405242e+08, dtype=float32), array(21323036., dtype=float32)]\n",
      "loss2 [array(32935.766, dtype=float32), array(38434.95, dtype=float32), array(39144.79, dtype=float32), array(40702.07, dtype=float32), array(47788.33, dtype=float32), array(42790.89, dtype=float32), array(45965.98, dtype=float32), array(50079.445, dtype=float32), array(43735.42, dtype=float32), array(43643.734, dtype=float32)]\n",
      "---> [Mean Time for Edge E1:\n",
      "ordinary   ---    55.1667\n",
      "connected  ---    56.9091\n",
      "All        ---    55.6581\n",
      ", Mean Fuel Consumption for Edge E1:\n",
      "ordinary   --- 93475.3428\n",
      "connected  --- 108593.0025\n",
      "All        --- 97739.2981\n",
      "]\n",
      " Retrying in 1 seconds\n",
      "noise_rate= 0.5135620000000001\n",
      "global_step= 7014 Total reward in episode 13 = -639046.4978873003\n",
      "loss1 [array(96280000., dtype=float32), array(1.0547896e+08, dtype=float32), array(66208324., dtype=float32), array(42831112., dtype=float32), array(20451084., dtype=float32), array(34151592., dtype=float32), array(27462704., dtype=float32), array(1.34896e+08, dtype=float32), array(16755671., dtype=float32), array(31264038., dtype=float32)]\n",
      "loss2 [array(45072.875, dtype=float32), array(42385.04, dtype=float32), array(41778.78, dtype=float32), array(40701.426, dtype=float32), array(42355.785, dtype=float32), array(45720.83, dtype=float32), array(51145.242, dtype=float32), array(47434.52, dtype=float32), array(42706.434, dtype=float32), array(41940.297, dtype=float32)]\n",
      "---> [Mean Time for Edge E1:\n",
      "ordinary   ---    54.7600\n",
      "connected  ---    58.7000\n",
      "All        ---    55.8857\n",
      ", Mean Fuel Consumption for Edge E1:\n",
      "ordinary   --- 91086.7180\n",
      "connected  --- 108333.9684\n",
      "All        --- 96014.5038\n",
      "]\n",
      " Retrying in 1 seconds\n",
      "noise_rate= 0.48383600000000004\n",
      "global_step= 7515 Total reward in episode 14 = -634046.9308404342\n",
      "loss1 [array(15236673., dtype=float32), array(1.3535653e+08, dtype=float32), array(60193692., dtype=float32), array(24934112., dtype=float32), array(1.2802083e+08, dtype=float32), array(27339286., dtype=float32), array(1.08298664e+08, dtype=float32), array(20766204., dtype=float32), array(2.8162736e+08, dtype=float32), array(69779424., dtype=float32)]\n",
      "loss2 [array(41198.32, dtype=float32), array(40630.4, dtype=float32), array(50285.67, dtype=float32), array(42462.277, dtype=float32), array(47201.633, dtype=float32), array(55218.156, dtype=float32), array(49379.5, dtype=float32), array(49948.47, dtype=float32), array(49008.594, dtype=float32), array(47849.645, dtype=float32)]\n",
      "---> [Mean Time for Edge E1:\n",
      "ordinary   ---    55.4943\n",
      "connected  ---    56.1379\n",
      "All        ---    55.6552\n",
      ", Mean Fuel Consumption for Edge E1:\n",
      "ordinary   --- 94296.2027\n",
      "connected  --- 105565.7395\n",
      "All        --- 97113.5869\n",
      "]\n",
      " Retrying in 1 seconds\n",
      "noise_rate= 0.45411\n",
      "global_step= 8016 Total reward in episode 15 = -622811.3209553732\n",
      "loss1 [array(39428024., dtype=float32), array(4.479896e+08, dtype=float32), array(1.8831565e+08, dtype=float32), array(3.5052544e+08, dtype=float32), array(56215980., dtype=float32), array(80024008., dtype=float32), array(2.3511149e+08, dtype=float32), array(1.2646443e+08, dtype=float32), array(1.6366954e+08, dtype=float32), array(3.536571e+08, dtype=float32)]\n",
      "loss2 [array(70973., dtype=float32), array(74919.336, dtype=float32), array(84045.56, dtype=float32), array(78926.234, dtype=float32), array(78609.06, dtype=float32), array(76286.64, dtype=float32), array(79856.81, dtype=float32), array(93286.33, dtype=float32), array(74775.766, dtype=float32), array(60041.055, dtype=float32)]\n",
      "---> [Mean Time for Edge E1:\n",
      "ordinary   ---    56.0230\n",
      "connected  ---    55.7222\n",
      "All        ---    55.9350\n",
      ", Mean Fuel Consumption for Edge E1:\n",
      "ordinary   --- 95785.8390\n",
      "connected  --- 102988.3089\n",
      "All        --- 97893.8790\n",
      "]\n",
      " Retrying in 1 seconds\n",
      "noise_rate= 0.42438400000000004\n",
      "global_step= 8517 Total reward in episode 16 = -682945.4416116601\n",
      "loss1 [array(1.068397e+08, dtype=float32), array(53323728., dtype=float32), array(63181928., dtype=float32), array(99968232., dtype=float32), array(77478104., dtype=float32), array(88146264., dtype=float32), array(1.6317122e+08, dtype=float32), array(73808024., dtype=float32), array(33120162., dtype=float32), array(1.09896824e+08, dtype=float32)]\n",
      "loss2 [array(64016.902, dtype=float32), array(61193.812, dtype=float32), array(63686.855, dtype=float32), array(57827.83, dtype=float32), array(70767.94, dtype=float32), array(75213.625, dtype=float32), array(71023.8, dtype=float32), array(66170.67, dtype=float32), array(68005.93, dtype=float32), array(62657.1, dtype=float32)]\n",
      "---> [Mean Time for Edge E1:\n",
      "ordinary   ---    54.6923\n",
      "connected  ---    54.5517\n",
      "All        ---    54.6542\n",
      ", Mean Fuel Consumption for Edge E1:\n",
      "ordinary   --- 93388.5395\n",
      "connected  --- 102329.1000\n",
      "All        --- 95811.6821\n",
      "]\n",
      " Retrying in 1 seconds\n",
      "noise_rate= 0.39465800000000006\n",
      "global_step= 9018 Total reward in episode 17 = -665220.9190981359\n",
      "loss1 [array(86373680., dtype=float32), array(36820048., dtype=float32), array(74220680., dtype=float32), array(60942860., dtype=float32), array(1.2946459e+08, dtype=float32), array(39917800., dtype=float32), array(1.10005496e+08, dtype=float32), array(19208098., dtype=float32), array(1.2818546e+08, dtype=float32), array(57053264., dtype=float32)]\n",
      "loss2 [array(60047.89, dtype=float32), array(67616.45, dtype=float32), array(63714.78, dtype=float32), array(61892.438, dtype=float32), array(56527.06, dtype=float32), array(54610.023, dtype=float32), array(48431.2, dtype=float32), array(54250.562, dtype=float32), array(52022.008, dtype=float32), array(50756.727, dtype=float32)]\n",
      "---> [Mean Time for Edge E1:\n",
      "ordinary   ---    53.6000\n",
      "connected  ---    56.3143\n",
      "All        ---    54.5048\n",
      ", Mean Fuel Consumption for Edge E1:\n",
      "ordinary   --- 92849.3228\n",
      "connected  --- 105314.1844\n",
      "All        --- 97004.2767\n",
      "]\n",
      " Retrying in 1 seconds\n",
      "noise_rate= 0.36493200000000003\n",
      "global_step= 9519 Total reward in episode 18 = -629520.1220136372\n",
      "loss1 [array(1.5506029e+08, dtype=float32), array(33045632., dtype=float32), array(43959192., dtype=float32), array(76763960., dtype=float32), array(5.448475e+08, dtype=float32), array(32823116., dtype=float32), array(1.33949096e+08, dtype=float32), array(1.3076758e+08, dtype=float32), array(53368988., dtype=float32), array(45042572., dtype=float32)]\n",
      "loss2 [array(85728.52, dtype=float32), array(93934.81, dtype=float32), array(100071.625, dtype=float32), array(114191.42, dtype=float32), array(105587.13, dtype=float32), array(98753.65, dtype=float32), array(99608.67, dtype=float32), array(88193.54, dtype=float32), array(76601.83, dtype=float32), array(76242.61, dtype=float32)]\n",
      "---> [Mean Time for Edge E1:\n",
      "ordinary   ---    54.5672\n",
      "connected  ---    56.7714\n",
      "All        ---    55.3235\n",
      ", Mean Fuel Consumption for Edge E1:\n",
      "ordinary   --- 90393.7822\n",
      "connected  --- 100518.5649\n",
      "All        --- 93867.9723\n",
      "]\n",
      " Retrying in 1 seconds\n",
      "noise_rate= 0.335206\n",
      "global_step= 10020 Total reward in episode 19 = -634447.7331619633\n",
      "loss1 [array(45997288., dtype=float32), array(1.3128837e+08, dtype=float32), array(94591376., dtype=float32), array(1.484184e+08, dtype=float32), array(2.6562805e+08, dtype=float32), array(8.562274e+08, dtype=float32), array(1.2677382e+08, dtype=float32), array(44959792., dtype=float32), array(2.350754e+08, dtype=float32), array(1.0829363e+08, dtype=float32)]\n",
      "loss2 [array(109932.49, dtype=float32), array(110437.49, dtype=float32), array(119899.984, dtype=float32), array(139607.03, dtype=float32), array(126106.16, dtype=float32), array(117256.22, dtype=float32), array(91063.93, dtype=float32), array(83776.195, dtype=float32), array(76134.75, dtype=float32), array(71793.15, dtype=float32)]\n",
      "---> [Mean Time for Edge E1:\n",
      "ordinary   ---    53.0649\n",
      "connected  ---    57.0000\n",
      "All        ---    54.1415\n",
      ", Mean Fuel Consumption for Edge E1:\n",
      "ordinary   --- 90289.8774\n",
      "connected  --- 106444.5025\n",
      "All        --- 94709.5390\n",
      "]\n",
      " Retrying in 1 seconds\n",
      "noise_rate= 0.3054800000000001\n",
      "global_step= 10521 Total reward in episode 20 = -582849.1720726548\n",
      "loss1 [array(54460488., dtype=float32), array(31512912., dtype=float32), array(18408350., dtype=float32), array(28250016., dtype=float32), array(34032368., dtype=float32), array(46141164., dtype=float32), array(1.0876437e+08, dtype=float32), array(75712032., dtype=float32), array(1.0194581e+08, dtype=float32), array(91184128., dtype=float32)]\n",
      "loss2 [array(65724.53, dtype=float32), array(69758.05, dtype=float32), array(68219., dtype=float32), array(69721.5, dtype=float32), array(77173.64, dtype=float32), array(82145.734, dtype=float32), array(89322.11, dtype=float32), array(88979.7, dtype=float32), array(99529.22, dtype=float32), array(111603.375, dtype=float32)]\n",
      "---> [Mean Time for Edge E1:\n",
      "ordinary   ---    53.9104\n",
      "connected  ---    55.7308\n",
      "All        ---    54.4194\n",
      ", Mean Fuel Consumption for Edge E1:\n",
      "ordinary   --- 90040.1548\n",
      "connected  --- 105221.1400\n",
      "All        --- 94284.3012\n",
      "]\n",
      " Retrying in 1 seconds\n",
      "noise_rate= 0.27575399999999994\n",
      "global_step= 11022 Total reward in episode 21 = -655690.1554971576\n",
      "loss1 [array(42880288., dtype=float32), array(64209236., dtype=float32), array(83611280., dtype=float32), array(38447976., dtype=float32), array(18957770., dtype=float32), array(1.2639604e+08, dtype=float32), array(42802288., dtype=float32), array(27748902., dtype=float32), array(78394760., dtype=float32), array(26013124., dtype=float32)]\n",
      "loss2 [array(79071.47, dtype=float32), array(79822.58, dtype=float32), array(79205.66, dtype=float32), array(80477.14, dtype=float32), array(81800.2, dtype=float32), array(76881.19, dtype=float32), array(77385.914, dtype=float32), array(79162.82, dtype=float32), array(78176.72, dtype=float32), array(76873.41, dtype=float32)]\n",
      "---> [Mean Time for Edge E1:\n",
      "ordinary   ---    55.0130\n",
      "connected  ---    54.1538\n",
      "All        ---    54.7961\n",
      ", Mean Fuel Consumption for Edge E1:\n",
      "ordinary   --- 91988.1174\n",
      "connected  --- 100398.7604\n",
      "All        --- 94111.1923\n",
      "]\n",
      " Retrying in 1 seconds\n",
      "noise_rate= 0.24602800000000002\n",
      "global_step= 11523 Total reward in episode 22 = -639041.863782568\n",
      "loss1 [array(33647168., dtype=float32), array(24359612., dtype=float32), array(54778232., dtype=float32), array(20614482., dtype=float32), array(54271824., dtype=float32), array(31783220., dtype=float32), array(19906194., dtype=float32), array(22153496., dtype=float32), array(1.6735179e+08, dtype=float32), array(2.0332755e+08, dtype=float32)]\n",
      "loss2 [array(61436.742, dtype=float32), array(64577.6, dtype=float32), array(70241.836, dtype=float32), array(73345.72, dtype=float32), array(78564.914, dtype=float32), array(79607.22, dtype=float32), array(83786.54, dtype=float32), array(88580.555, dtype=float32), array(90525.46, dtype=float32), array(83815.66, dtype=float32)]\n",
      "---> [Mean Time for Edge E1:\n",
      "ordinary   ---    54.1831\n",
      "connected  ---    56.4722\n",
      "All        ---    54.9533\n",
      ", Mean Fuel Consumption for Edge E1:\n",
      "ordinary   --- 94070.0923\n",
      "connected  --- 106557.1455\n",
      "All        --- 98271.3438\n",
      "]\n",
      " Retrying in 1 seconds\n",
      "noise_rate= 0.2163020000000001\n",
      "global_step= 12024 Total reward in episode 23 = -609592.9928446871\n",
      "loss1 [array(69545952., dtype=float32), array(1.4523867e+08, dtype=float32), array(73820288., dtype=float32), array(95353456., dtype=float32), array(42607912., dtype=float32), array(43467276., dtype=float32), array(33107068., dtype=float32), array(20781484., dtype=float32), array(1.0982411e+08, dtype=float32), array(35599988., dtype=float32)]\n",
      "loss2 [array(90342.44, dtype=float32), array(82576.516, dtype=float32), array(83992.76, dtype=float32), array(73242.34, dtype=float32), array(75029.375, dtype=float32), array(78081.62, dtype=float32), array(76114.86, dtype=float32), array(75754.516, dtype=float32), array(85737.555, dtype=float32), array(86150.375, dtype=float32)]\n",
      "---> [Mean Time for Edge E1:\n",
      "ordinary   ---    55.5647\n",
      "connected  ---    58.8621\n",
      "All        ---    56.4035\n",
      ", Mean Fuel Consumption for Edge E1:\n",
      "ordinary   --- 94504.0436\n",
      "connected  --- 112094.9355\n",
      "All        --- 98978.9196\n",
      "]\n",
      " Retrying in 1 seconds\n",
      "noise_rate= 0.18657599999999996\n",
      "global_step= 12525 Total reward in episode 24 = -627251.0214233295\n",
      "loss1 [array(44589064., dtype=float32), array(73876184., dtype=float32), array(59281620., dtype=float32), array(22562788., dtype=float32), array(51494048., dtype=float32), array(50412208., dtype=float32), array(54733248., dtype=float32), array(49902512., dtype=float32), array(33491214., dtype=float32), array(28929848., dtype=float32)]\n",
      "loss2 [array(70110.68, dtype=float32), array(71083.94, dtype=float32), array(78900.586, dtype=float32), array(75148.305, dtype=float32), array(77791.67, dtype=float32), array(76181.32, dtype=float32), array(81245.67, dtype=float32), array(81471.23, dtype=float32), array(74951.94, dtype=float32), array(80281.84, dtype=float32)]\n",
      "---> [Mean Time for Edge E1:\n",
      "ordinary   ---    55.2879\n",
      "connected  ---    53.5200\n",
      "All        ---    54.8022\n",
      ", Mean Fuel Consumption for Edge E1:\n",
      "ordinary   --- 94629.0537\n",
      "connected  --- 101605.4806\n",
      "All        --- 96545.6545\n",
      "]\n",
      " Retrying in 1 seconds\n",
      "noise_rate= 0.15685000000000004\n",
      "global_step= 13026 Total reward in episode 25 = -642089.517068325\n",
      "loss1 [array(25446188., dtype=float32), array(35594268., dtype=float32), array(69302232., dtype=float32), array(59109088., dtype=float32), array(43495264., dtype=float32), array(48561460., dtype=float32), array(78817424., dtype=float32), array(1.2588973e+08, dtype=float32), array(30006042., dtype=float32), array(1.466214e+08, dtype=float32)]\n",
      "loss2 [array(71686.75, dtype=float32), array(76322.89, dtype=float32), array(74764.945, dtype=float32), array(72367.21, dtype=float32), array(82348.53, dtype=float32), array(83180.38, dtype=float32), array(84705.49, dtype=float32), array(84676.54, dtype=float32), array(88883.32, dtype=float32), array(94381.89, dtype=float32)]\n",
      "---> [Mean Time for Edge E1:\n",
      "ordinary   ---    53.6528\n",
      "connected  ---    55.5806\n",
      "All        ---    54.2330\n",
      ", Mean Fuel Consumption for Edge E1:\n",
      "ordinary   --- 89250.5222\n",
      "connected  --- 98846.2851\n",
      "All        --- 92138.5674\n",
      "]\n",
      " Retrying in 1 seconds\n",
      "noise_rate= 0.12712400000000001\n",
      "global_step= 13527 Total reward in episode 26 = -705242.7481463448\n",
      "loss1 [array(1.6947442e+08, dtype=float32), array(1.0843666e+08, dtype=float32), array(41865824., dtype=float32), array(1.2069757e+08, dtype=float32), array(51786752., dtype=float32), array(55390696., dtype=float32), array(62073844., dtype=float32), array(86081672., dtype=float32), array(79962840., dtype=float32), array(87900168., dtype=float32)]\n",
      "loss2 [array(127408.7, dtype=float32), array(125311.49, dtype=float32), array(122174.05, dtype=float32), array(120191.55, dtype=float32), array(117830.69, dtype=float32), array(113916.49, dtype=float32), array(117789.53, dtype=float32), array(119246.36, dtype=float32), array(122592.22, dtype=float32), array(123368.695, dtype=float32)]\n",
      "---> [Mean Time for Edge E1:\n",
      "ordinary   ---    54.8919\n",
      "connected  ---    56.0000\n",
      "All        ---    55.2336\n",
      ", Mean Fuel Consumption for Edge E1:\n",
      "ordinary   --- 94929.8275\n",
      "connected  --- 105876.6578\n",
      "All        --- 98305.9527\n",
      "]\n",
      " Retrying in 1 seconds\n",
      "noise_rate= 0.09739799999999998\n",
      "global_step= 14028 Total reward in episode 27 = -631651.0973883073\n",
      "loss1 [array(42910532., dtype=float32), array(32923900., dtype=float32), array(2.1135226e+08, dtype=float32), array(35918672., dtype=float32), array(93419240., dtype=float32), array(1.2147852e+08, dtype=float32), array(20335238., dtype=float32), array(84838752., dtype=float32), array(51120920., dtype=float32), array(41091140., dtype=float32)]\n",
      "loss2 [array(97416.69, dtype=float32), array(97098.86, dtype=float32), array(99820.63, dtype=float32), array(93360.77, dtype=float32), array(88323.516, dtype=float32), array(89614.17, dtype=float32), array(93237.89, dtype=float32), array(90180.31, dtype=float32), array(84851.24, dtype=float32), array(85475.8, dtype=float32)]\n",
      "---> [Mean Time for Edge E1:\n",
      "ordinary   ---    55.5256\n",
      "connected  ---    55.5357\n",
      "All        ---    55.5283\n",
      ", Mean Fuel Consumption for Edge E1:\n",
      "ordinary   --- 96891.0330\n",
      "connected  --- 102554.8154\n",
      "All        --- 98387.1265\n",
      "]\n",
      " Retrying in 1 seconds\n",
      "noise_rate= 0.06767200000000007\n",
      "global_step= 14529 Total reward in episode 28 = -673240.4016523985\n",
      "loss1 [array(57307928., dtype=float32), array(28384128., dtype=float32), array(20662374., dtype=float32), array(23338668., dtype=float32), array(63888424., dtype=float32), array(19403398., dtype=float32), array(25541148., dtype=float32), array(55541928., dtype=float32), array(50892752., dtype=float32), array(61486856., dtype=float32)]\n",
      "loss2 [array(75167.78, dtype=float32), array(75104.97, dtype=float32), array(76851.266, dtype=float32), array(75950.48, dtype=float32), array(79323.445, dtype=float32), array(81788.81, dtype=float32), array(81973.68, dtype=float32), array(80781.125, dtype=float32), array(84917.484, dtype=float32), array(88300.7, dtype=float32)]\n",
      "---> [Mean Time for Edge E1:\n",
      "ordinary   ---    54.7164\n",
      "connected  ---    57.5116\n",
      "All        ---    55.8091\n",
      ", Mean Fuel Consumption for Edge E1:\n",
      "ordinary   --- 95997.6298\n",
      "connected  --- 109882.3011\n",
      "All        --- 101425.2740\n",
      "]\n",
      " Retrying in 1 seconds\n",
      "noise_rate= 0.037946000000000035\n",
      "global_step= 15030 Total reward in episode 29 = -642963.8510661168\n",
      "loss1 [array(29587858., dtype=float32), array(48920004., dtype=float32), array(35136548., dtype=float32), array(29132002., dtype=float32), array(87485536., dtype=float32), array(57593184., dtype=float32), array(60237024., dtype=float32), array(41028956., dtype=float32), array(1.0158296e+08, dtype=float32), array(1.6867848e+08, dtype=float32)]\n",
      "loss2 [array(95243.03, dtype=float32), array(100174.84, dtype=float32), array(100181.91, dtype=float32), array(105788.2, dtype=float32), array(106468.516, dtype=float32), array(104018.44, dtype=float32), array(107948.25, dtype=float32), array(110012.234, dtype=float32), array(112613.914, dtype=float32), array(113974.94, dtype=float32)]\n",
      "---> [Mean Time for Edge E1:\n",
      "ordinary   ---    54.3582\n",
      "connected  ---    56.7429\n",
      "All        ---    55.1765\n",
      ", Mean Fuel Consumption for Edge E1:\n",
      "ordinary   --- 92569.6848\n",
      "connected  --- 108088.4321\n",
      "All        --- 97894.7451\n",
      "]\n",
      " Retrying in 1 seconds\n",
      "noise_rate= 0.01\n",
      "global_step= 15531 Total reward in episode 30 = -656251.378288677\n",
      "loss1 [array(36727320., dtype=float32), array(29756380., dtype=float32), array(40187308., dtype=float32), array(36457196., dtype=float32), array(19670380., dtype=float32), array(22981588., dtype=float32), array(87682320., dtype=float32), array(45457232., dtype=float32), array(74344832., dtype=float32), array(1.9459034e+08, dtype=float32)]\n",
      "loss2 [array(86703.39, dtype=float32), array(91723.375, dtype=float32), array(96254.06, dtype=float32), array(98708.44, dtype=float32), array(105174.3, dtype=float32), array(103044.22, dtype=float32), array(107992.5, dtype=float32), array(110159.65, dtype=float32), array(113257.516, dtype=float32), array(113640.53, dtype=float32)]\n",
      "---> [Mean Time for Edge E1:\n",
      "ordinary   ---    54.3871\n",
      "connected  ---    57.1364\n",
      "All        ---    55.5283\n",
      ", Mean Fuel Consumption for Edge E1:\n",
      "ordinary   --- 93372.8796\n",
      "connected  --- 102877.5764\n",
      "All        --- 97318.2254\n",
      "]\n",
      " Retrying in 1 seconds\n",
      "noise_rate= 0.01\n",
      "global_step= 16032 Total reward in episode 31 = -595850.9447823868\n",
      "loss1 [array(77622016., dtype=float32), array(36522960., dtype=float32), array(72879984., dtype=float32), array(72298120., dtype=float32), array(61106964., dtype=float32), array(44565028., dtype=float32), array(2.8563635e+08, dtype=float32), array(60342340., dtype=float32), array(39052016., dtype=float32), array(55909464., dtype=float32)]\n",
      "loss2 [array(139504.42, dtype=float32), array(142259.75, dtype=float32), array(143513.75, dtype=float32), array(136615.73, dtype=float32), array(136272.27, dtype=float32), array(138991.69, dtype=float32), array(137404.81, dtype=float32), array(133928.6, dtype=float32), array(128955.08, dtype=float32), array(123373.016, dtype=float32)]\n",
      "---> [Mean Time for Edge E1:\n",
      "ordinary   ---    54.3889\n",
      "connected  ---    57.7838\n",
      "All        ---    55.5413\n",
      ", Mean Fuel Consumption for Edge E1:\n",
      "ordinary   --- 91823.3887\n",
      "connected  --- 101038.8020\n",
      "All        --- 94951.5565\n",
      "]\n",
      " Retrying in 1 seconds\n",
      "noise_rate= 0.01\n",
      "global_step= 16533 Total reward in episode 32 = -625350.8484239659\n",
      "loss1 [array(2.1992616e+08, dtype=float32), array(62054476., dtype=float32), array(72116576., dtype=float32), array(1.7555008e+08, dtype=float32), array(1.17892856e+08, dtype=float32), array(39507152., dtype=float32), array(43472920., dtype=float32), array(39916916., dtype=float32), array(67208400., dtype=float32), array(2.1841533e+08, dtype=float32)]\n",
      "loss2 [array(118476.42, dtype=float32), array(117001.76, dtype=float32), array(118131.68, dtype=float32), array(115977., dtype=float32), array(110775.54, dtype=float32), array(110477.86, dtype=float32), array(109967.234, dtype=float32), array(105694.125, dtype=float32), array(103552.41, dtype=float32), array(99228.55, dtype=float32)]\n",
      "---> [Mean Time for Edge E1:\n",
      "ordinary   ---    55.0000\n",
      "connected  ---    56.6111\n",
      "All        ---    55.5524\n",
      ", Mean Fuel Consumption for Edge E1:\n",
      "ordinary   --- 92159.4283\n",
      "connected  --- 100283.1687\n",
      "All        --- 94944.7107\n",
      "]\n",
      " Retrying in 1 seconds\n",
      "noise_rate= 0.01\n",
      "global_step= 17034 Total reward in episode 33 = -678634.3100569105\n",
      "loss1 [array(61872332., dtype=float32), array(59417704., dtype=float32), array(28129760., dtype=float32), array(52481664., dtype=float32), array(46425252., dtype=float32), array(50331900., dtype=float32), array(23517644., dtype=float32), array(34267208., dtype=float32), array(54571940., dtype=float32), array(24461404., dtype=float32)]\n",
      "loss2 [array(91571., dtype=float32), array(92361.69, dtype=float32), array(96245.95, dtype=float32), array(93010.2, dtype=float32), array(97239.51, dtype=float32), array(99438.805, dtype=float32), array(99275.86, dtype=float32), array(100311.05, dtype=float32), array(104613.39, dtype=float32), array(97930.69, dtype=float32)]\n",
      "---> [Mean Time for Edge E1:\n",
      "ordinary   ---    55.4937\n",
      "connected  ---    57.2857\n",
      "All        ---    56.0439\n",
      ", Mean Fuel Consumption for Edge E1:\n",
      "ordinary   --- 95549.9656\n",
      "connected  --- 108580.3831\n",
      "All        --- 99550.5324\n",
      "]\n",
      " Retrying in 1 seconds\n",
      "noise_rate= 0.01\n",
      "global_step= 17535 Total reward in episode 34 = -650472.7333093543\n",
      "loss1 [array(55869836., dtype=float32), array(24439744., dtype=float32), array(95611608., dtype=float32), array(24474196., dtype=float32), array(48656408., dtype=float32), array(42099592., dtype=float32), array(45005140., dtype=float32), array(37110404., dtype=float32), array(82333376., dtype=float32), array(48733472., dtype=float32)]\n",
      "loss2 [array(109574.25, dtype=float32), array(107674.04, dtype=float32), array(107699.65, dtype=float32), array(100305.53, dtype=float32), array(101800.83, dtype=float32), array(101439.28, dtype=float32), array(104918.26, dtype=float32), array(102504.41, dtype=float32), array(106892.54, dtype=float32), array(104134.56, dtype=float32)]\n",
      "---> [Mean Time for Edge E1:\n",
      "ordinary   ---    55.4810\n",
      "connected  ---    57.2571\n",
      "All        ---    56.0263\n",
      ", Mean Fuel Consumption for Edge E1:\n",
      "ordinary   --- 95717.1613\n",
      "connected  --- 109382.1137\n",
      "All        --- 99912.5414\n",
      "]\n",
      " Retrying in 1 seconds\n",
      "noise_rate= 0.01\n",
      "global_step= 18036 Total reward in episode 35 = -657265.6758698871\n",
      "loss1 [array(44668840., dtype=float32), array(35974472., dtype=float32), array(57504196., dtype=float32), array(46892132., dtype=float32), array(15002356., dtype=float32), array(73470632., dtype=float32), array(59929724., dtype=float32), array(34440516., dtype=float32), array(2.4952056e+08, dtype=float32), array(55930952., dtype=float32)]\n",
      "loss2 [array(92795.59, dtype=float32), array(91633.91, dtype=float32), array(94157.766, dtype=float32), array(93856.67, dtype=float32), array(98238.48, dtype=float32), array(97974., dtype=float32), array(100554.484, dtype=float32), array(101080.35, dtype=float32), array(103995.67, dtype=float32), array(102660.984, dtype=float32)]\n",
      "---> [Mean Time for Edge E1:\n",
      "ordinary   ---    54.9710\n",
      "connected  ---    56.3889\n",
      "All        ---    55.4571\n",
      ", Mean Fuel Consumption for Edge E1:\n",
      "ordinary   --- 92239.9726\n",
      "connected  --- 101052.7662\n",
      "All        --- 95261.5018\n",
      "]\n",
      " Retrying in 1 seconds\n",
      "noise_rate= 0.01\n",
      "global_step= 18537 Total reward in episode 36 = -656495.7287519486\n",
      "loss1 [array(1.0900272e+08, dtype=float32), array(1.840741e+08, dtype=float32), array(43102520., dtype=float32), array(1.501903e+08, dtype=float32), array(93451040., dtype=float32), array(1.18410616e+08, dtype=float32), array(99748384., dtype=float32), array(4.5429898e+08, dtype=float32), array(3.4850394e+08, dtype=float32), array(1.771846e+08, dtype=float32)]\n",
      "loss2 [array(150650.9, dtype=float32), array(151360.6, dtype=float32), array(154265.44, dtype=float32), array(155961.4, dtype=float32), array(168389.11, dtype=float32), array(171081.72, dtype=float32), array(170822.5, dtype=float32), array(175753.56, dtype=float32), array(167057.16, dtype=float32), array(154069.66, dtype=float32)]\n",
      "---> [Mean Time for Edge E1:\n",
      "ordinary   ---    54.3871\n",
      "connected  ---    57.1364\n",
      "All        ---    55.5283\n",
      ", Mean Fuel Consumption for Edge E1:\n",
      "ordinary   --- 93380.7619\n",
      "connected  --- 102888.6832\n",
      "All        --- 97327.4462\n",
      "]\n",
      " Retrying in 1 seconds\n",
      "noise_rate= 0.01\n",
      "global_step= 19038 Total reward in episode 37 = -625350.8484239659\n",
      "loss1 [array(1.1723613e+08, dtype=float32), array(1.201232e+08, dtype=float32), array(13623701., dtype=float32), array(27354506., dtype=float32), array(17164040., dtype=float32), array(14741312., dtype=float32), array(25932822., dtype=float32), array(23834956., dtype=float32), array(25986736., dtype=float32), array(15021084., dtype=float32)]\n",
      "loss2 [array(98974.42, dtype=float32), array(91608.66, dtype=float32), array(92111.17, dtype=float32), array(89246.7, dtype=float32), array(87547.625, dtype=float32), array(95525.66, dtype=float32), array(91511.75, dtype=float32), array(91558.39, dtype=float32), array(88445.375, dtype=float32), array(86615.3, dtype=float32)]\n",
      "---> [Mean Time for Edge E1:\n",
      "ordinary   ---    55.0000\n",
      "connected  ---    56.6111\n",
      "All        ---    55.5524\n",
      ", Mean Fuel Consumption for Edge E1:\n",
      "ordinary   --- 92159.4283\n",
      "connected  --- 100283.1687\n",
      "All        --- 94944.7107\n",
      "]\n",
      " Retrying in 1 seconds\n",
      "noise_rate= 0.01\n",
      "global_step= 19539 Total reward in episode 38 = -626363.676545155\n",
      "loss1 [array(31618498., dtype=float32), array(39449268., dtype=float32), array(39480980., dtype=float32), array(46905336., dtype=float32), array(37227056., dtype=float32), array(57042112., dtype=float32), array(2.9353926e+08, dtype=float32), array(35433632., dtype=float32), array(25765678., dtype=float32), array(81196408., dtype=float32)]\n",
      "loss2 [array(125554.19, dtype=float32), array(124158.25, dtype=float32), array(130205.08, dtype=float32), array(133472.69, dtype=float32), array(131090.67, dtype=float32), array(134956.75, dtype=float32), array(131924.11, dtype=float32), array(131948.94, dtype=float32), array(135797.78, dtype=float32), array(126435.27, dtype=float32)]\n",
      "---> [Mean Time for Edge E1:\n",
      "ordinary   ---    55.0000\n",
      "connected  ---    56.6111\n",
      "All        ---    55.5524\n",
      ", Mean Fuel Consumption for Edge E1:\n",
      "ordinary   --- 92386.5043\n",
      "connected  --- 100234.8793\n",
      "All        --- 95077.3757\n",
      "]\n",
      " Retrying in 1 seconds\n",
      "noise_rate= 0.01\n",
      "global_step= 20040 Total reward in episode 39 = -677590.5752083119\n",
      "loss1 [array(10706762., dtype=float32), array(9273122., dtype=float32), array(16058731., dtype=float32), array(39414440., dtype=float32), array(43880584., dtype=float32), array(25909236., dtype=float32), array(37541520., dtype=float32), array(34811788., dtype=float32), array(73090888., dtype=float32), array(44547800., dtype=float32)]\n",
      "loss2 [array(92213.32, dtype=float32), array(93952.11, dtype=float32), array(100127.82, dtype=float32), array(100502.67, dtype=float32), array(106334.78, dtype=float32), array(110101.58, dtype=float32), array(113537.86, dtype=float32), array(114368.64, dtype=float32), array(113224.33, dtype=float32), array(122898.914, dtype=float32)]\n",
      "---> [Mean Time for Edge E1:\n",
      "ordinary   ---    55.7500\n",
      "connected  ---    55.9070\n",
      "All        ---    55.8108\n",
      ", Mean Fuel Consumption for Edge E1:\n",
      "ordinary   --- 94115.2799\n",
      "connected  --- 99739.6664\n",
      "All        --- 96294.0963\n",
      "]\n",
      " Retrying in 1 seconds\n",
      "noise_rate= 0.01\n",
      "global_step= 20541 Total reward in episode 40 = -625358.8440175666\n",
      "loss1 [array(25575798., dtype=float32), array(43518068., dtype=float32), array(32959932., dtype=float32), array(1.10785016e+08, dtype=float32), array(21208722., dtype=float32), array(21366974., dtype=float32), array(26108904., dtype=float32), array(22127774., dtype=float32), array(64776928., dtype=float32), array(25108822., dtype=float32)]\n",
      "loss2 [array(94120.84, dtype=float32), array(96172.695, dtype=float32), array(91007.11, dtype=float32), array(90218.164, dtype=float32), array(92995.734, dtype=float32), array(94632.75, dtype=float32), array(89404.39, dtype=float32), array(91286.28, dtype=float32), array(94595.57, dtype=float32), array(90186.14, dtype=float32)]\n",
      "---> [Mean Time for Edge E1:\n",
      "ordinary   ---    55.0000\n",
      "connected  ---    56.6111\n",
      "All        ---    55.5524\n",
      ", Mean Fuel Consumption for Edge E1:\n",
      "ordinary   --- 92386.5043\n",
      "connected  --- 100234.8793\n",
      "All        --- 95077.3757\n",
      "]\n",
      " Retrying in 1 seconds\n",
      "noise_rate= 0.01\n",
      "global_step= 21042 Total reward in episode 41 = -644048.8901737615\n",
      "loss1 [array(29995754., dtype=float32), array(36228292., dtype=float32), array(67427208., dtype=float32), array(55124404., dtype=float32), array(22732554., dtype=float32), array(51826224., dtype=float32), array(33312450., dtype=float32), array(43246036., dtype=float32), array(44379676., dtype=float32), array(2.32487e+08, dtype=float32)]\n",
      "loss2 [array(116744.36, dtype=float32), array(111288.24, dtype=float32), array(110239.5, dtype=float32), array(112593.61, dtype=float32), array(106970.64, dtype=float32), array(108880.89, dtype=float32), array(108942.516, dtype=float32), array(112995.78, dtype=float32), array(108606.586, dtype=float32), array(107893.5, dtype=float32)]\n",
      "---> [Mean Time for Edge E1:\n",
      "ordinary   ---    55.1571\n",
      "connected  ---    56.4359\n",
      "All        ---    55.6147\n",
      ", Mean Fuel Consumption for Edge E1:\n",
      "ordinary   --- 95733.5491\n",
      "connected  --- 102186.5691\n",
      "All        --- 98042.4279\n",
      "]\n",
      " Retrying in 1 seconds\n",
      "noise_rate= 0.01\n",
      "global_step= 21543 Total reward in episode 42 = -656550.2289501353\n",
      "loss1 [array(37692476., dtype=float32), array(23232430., dtype=float32), array(87217824., dtype=float32), array(54707296., dtype=float32), array(69526384., dtype=float32), array(40172348., dtype=float32), array(39288696., dtype=float32), array(38732060., dtype=float32), array(3.210162e+08, dtype=float32), array(2.6213848e+08, dtype=float32)]\n",
      "loss2 [array(112615.94, dtype=float32), array(111764.47, dtype=float32), array(115487.734, dtype=float32), array(112732.46, dtype=float32), array(110437.36, dtype=float32), array(116315.97, dtype=float32), array(118372.05, dtype=float32), array(125106.89, dtype=float32), array(121658.19, dtype=float32), array(119625.66, dtype=float32)]\n",
      "---> [Mean Time for Edge E1:\n",
      "ordinary   ---    54.9710\n",
      "connected  ---    56.3889\n",
      "All        ---    55.4571\n",
      ", Mean Fuel Consumption for Edge E1:\n",
      "ordinary   --- 92012.8965\n",
      "connected  --- 101018.0906\n",
      "All        --- 95100.3916\n",
      "]\n",
      " Retrying in 1 seconds\n",
      "noise_rate= 0.01\n",
      "global_step= 22044 Total reward in episode 43 = -656251.378288677\n",
      "loss1 [array(73216400., dtype=float32), array(42545772., dtype=float32), array(35445728., dtype=float32), array(33468680., dtype=float32), array(32676176., dtype=float32), array(54094752., dtype=float32), array(2.7315946e+08, dtype=float32), array(2.3265733e+08, dtype=float32), array(22700144., dtype=float32), array(37100880., dtype=float32)]\n",
      "loss2 [array(128910.45, dtype=float32), array(126284.08, dtype=float32), array(124035.29, dtype=float32), array(124652.62, dtype=float32), array(122983.266, dtype=float32), array(125439.44, dtype=float32), array(125707.56, dtype=float32), array(121356.91, dtype=float32), array(109419.67, dtype=float32), array(110445.625, dtype=float32)]\n",
      "---> [Mean Time for Edge E1:\n",
      "ordinary   ---    54.3871\n",
      "connected  ---    57.1364\n",
      "All        ---    55.5283\n",
      ", Mean Fuel Consumption for Edge E1:\n",
      "ordinary   --- 93372.8796\n",
      "connected  --- 102877.5764\n",
      "All        --- 97318.2254\n",
      "]\n",
      " Retrying in 1 seconds\n",
      "noise_rate= 0.01\n",
      "global_step= 22545 Total reward in episode 44 = -667350.1675823705\n",
      "loss1 [array(15834167., dtype=float32), array(22154624., dtype=float32), array(15104470., dtype=float32), array(34879016., dtype=float32), array(15490151., dtype=float32), array(1.106303e+08, dtype=float32), array(25245328., dtype=float32), array(1.14804184e+08, dtype=float32), array(41266560., dtype=float32), array(26693062., dtype=float32)]\n",
      "loss2 [array(81696.53, dtype=float32), array(84086.42, dtype=float32), array(89995.26, dtype=float32), array(88527.48, dtype=float32), array(87131.99, dtype=float32), array(91544.29, dtype=float32), array(95896.82, dtype=float32), array(91387.83, dtype=float32), array(97056.37, dtype=float32), array(94648.31, dtype=float32)]\n",
      "---> [Mean Time for Edge E1:\n",
      "ordinary   ---    54.9444\n",
      "connected  ---    56.2857\n",
      "All        ---    55.4386\n",
      ", Mean Fuel Consumption for Edge E1:\n",
      "ordinary   --- 95437.3729\n",
      "connected  --- 101256.2533\n",
      "All        --- 97581.1709\n",
      "]\n",
      " Retrying in 1 seconds\n",
      "noise_rate= 0.01\n",
      "global_step= 23046 Total reward in episode 45 = -595850.9447823868\n",
      "loss1 [array(31209012., dtype=float32), array(26682188., dtype=float32), array(26964848., dtype=float32), array(23598212., dtype=float32), array(43180056., dtype=float32), array(36460616., dtype=float32), array(29739036., dtype=float32), array(17788632., dtype=float32), array(21527482., dtype=float32), array(9267556., dtype=float32)]\n",
      "loss2 [array(66324.05, dtype=float32), array(70946.32, dtype=float32), array(73801.47, dtype=float32), array(64262.113, dtype=float32), array(66833.19, dtype=float32), array(71423.78, dtype=float32), array(63575.36, dtype=float32), array(73269.266, dtype=float32), array(68103.6, dtype=float32), array(66575.87, dtype=float32)]\n",
      "---> [Mean Time for Edge E1:\n",
      "ordinary   ---    54.3889\n",
      "connected  ---    57.7838\n",
      "All        ---    55.5413\n",
      ", Mean Fuel Consumption for Edge E1:\n",
      "ordinary   --- 91823.3887\n",
      "connected  --- 101038.8020\n",
      "All        --- 94951.5565\n",
      "]\n",
      " Retrying in 1 seconds\n",
      "noise_rate= 0.01\n",
      "global_step= 23547 Total reward in episode 46 = -656210.406835969\n",
      "loss1 [array(31465224., dtype=float32), array(2.5383502e+08, dtype=float32), array(35186808., dtype=float32), array(46122460., dtype=float32), array(32087472., dtype=float32), array(61737216., dtype=float32), array(31261910., dtype=float32), array(40487616., dtype=float32), array(70130208., dtype=float32), array(47455384., dtype=float32)]\n",
      "loss2 [array(120084.42, dtype=float32), array(116220.69, dtype=float32), array(115840.266, dtype=float32), array(120982.56, dtype=float32), array(113477.234, dtype=float32), array(115183.42, dtype=float32), array(114506.35, dtype=float32), array(108477.94, dtype=float32), array(109715.97, dtype=float32), array(113463.5, dtype=float32)]\n",
      "---> [Mean Time for Edge E1:\n",
      "ordinary   ---    54.3871\n",
      "connected  ---    57.1364\n",
      "All        ---    55.5283\n",
      ", Mean Fuel Consumption for Edge E1:\n",
      "ordinary   --- 93372.8796\n",
      "connected  --- 102877.5764\n",
      "All        --- 97318.2254\n",
      "]\n",
      " Retrying in 1 seconds\n",
      "noise_rate= 0.01\n",
      "global_step= 24048 Total reward in episode 47 = -595850.9447823868\n",
      "loss1 [array(59332880., dtype=float32), array(38113444., dtype=float32), array(39153064., dtype=float32), array(34415428., dtype=float32), array(75033520., dtype=float32), array(84983752., dtype=float32), array(45494884., dtype=float32), array(39784820., dtype=float32), array(40773108., dtype=float32), array(43915628., dtype=float32)]\n",
      "loss2 [array(113696.84, dtype=float32), array(115680.19, dtype=float32), array(105354.7, dtype=float32), array(100843.44, dtype=float32), array(101912.055, dtype=float32), array(102715.56, dtype=float32), array(93624.23, dtype=float32), array(97082.94, dtype=float32), array(97463.92, dtype=float32), array(102070.984, dtype=float32)]\n",
      "---> [Mean Time for Edge E1:\n",
      "ordinary   ---    54.3889\n",
      "connected  ---    57.7838\n",
      "All        ---    55.5413\n",
      ", Mean Fuel Consumption for Edge E1:\n",
      "ordinary   --- 91823.3887\n",
      "connected  --- 101038.8020\n",
      "All        --- 94951.5565\n",
      "]\n",
      " Retrying in 1 seconds\n",
      "noise_rate= 0.01\n",
      "global_step= 24549 Total reward in episode 48 = -656558.224543736\n",
      "loss1 [array(36225928., dtype=float32), array(2.6871763e+08, dtype=float32), array(75298856., dtype=float32), array(83038672., dtype=float32), array(82765944., dtype=float32), array(33682424., dtype=float32), array(1.7709541e+08, dtype=float32), array(1.558125e+08, dtype=float32), array(56177832., dtype=float32), array(90843920., dtype=float32)]\n",
      "loss2 [array(149361.6, dtype=float32), array(141279.6, dtype=float32), array(139520.03, dtype=float32), array(132190.9, dtype=float32), array(132844.12, dtype=float32), array(129060.375, dtype=float32), array(127309.125, dtype=float32), array(113219.28, dtype=float32), array(118981.195, dtype=float32), array(119608.08, dtype=float32)]\n",
      "---> [Mean Time for Edge E1:\n",
      "ordinary   ---    54.9710\n",
      "connected  ---    56.3889\n",
      "All        ---    55.4571\n",
      ", Mean Fuel Consumption for Edge E1:\n",
      "ordinary   --- 92239.9726\n",
      "connected  --- 100969.8013\n",
      "All        --- 95233.0567\n",
      "]\n",
      " Retrying in 1 seconds\n",
      "noise_rate= 0.01\n",
      "global_step= 25050 Total reward in episode 49 = -656251.378288677\n",
      "loss1 [array(1.4093462e+08, dtype=float32), array(30626434., dtype=float32), array(94316232., dtype=float32), array(78614744., dtype=float32), array(39986812., dtype=float32), array(1.1783323e+08, dtype=float32), array(55390088., dtype=float32), array(93174800., dtype=float32), array(88455640., dtype=float32), array(30915820., dtype=float32)]\n",
      "loss2 [array(101906.55, dtype=float32), array(112826.914, dtype=float32), array(110275.93, dtype=float32), array(113213.21, dtype=float32), array(105593.586, dtype=float32), array(109363.32, dtype=float32), array(104954.8, dtype=float32), array(104239.31, dtype=float32), array(98030.98, dtype=float32), array(98696.98, dtype=float32)]\n",
      "---> [Mean Time for Edge E1:\n",
      "ordinary   ---    54.3871\n",
      "connected  ---    57.1364\n",
      "All        ---    55.5283\n",
      ", Mean Fuel Consumption for Edge E1:\n",
      "ordinary   --- 93372.8796\n",
      "connected  --- 102877.5764\n",
      "All        --- 97318.2254\n",
      "]\n",
      " Retrying in 1 seconds\n",
      "noise_rate= 0.01\n",
      "global_step= 25551 Total reward in episode 50 = -650065.1018298592\n",
      "loss1 [array(22007578., dtype=float32), array(21529070., dtype=float32), array(37916892., dtype=float32), array(1.390125e+08, dtype=float32), array(1.5396813e+08, dtype=float32), array(16062865., dtype=float32), array(36129676., dtype=float32), array(32102640., dtype=float32), array(44232668., dtype=float32), array(15752646., dtype=float32)]\n",
      "loss2 [array(100521.4, dtype=float32), array(110207.664, dtype=float32), array(107569.28, dtype=float32), array(104454.37, dtype=float32), array(107122.49, dtype=float32), array(105889.64, dtype=float32), array(103874.05, dtype=float32), array(103309.625, dtype=float32), array(101720.266, dtype=float32), array(102790.914, dtype=float32)]\n",
      "---> [Mean Time for Edge E1:\n",
      "ordinary   ---    54.4516\n",
      "connected  ---    57.0455\n",
      "All        ---    55.5283\n",
      ", Mean Fuel Consumption for Edge E1:\n",
      "ordinary   --- 93464.8859\n",
      "connected  --- 102928.0687\n",
      "All        --- 97392.9995\n",
      "]\n",
      " Retrying in 1 seconds\n",
      "noise_rate= 0.01\n",
      "global_step= 26052 Total reward in episode 51 = -595677.5653788194\n",
      "loss1 [array(99008832., dtype=float32), array(49764648., dtype=float32), array(32126822., dtype=float32), array(2.1056923e+08, dtype=float32), array(61292868., dtype=float32), array(50806616., dtype=float32), array(47668872., dtype=float32), array(33538392., dtype=float32), array(1.6833138e+08, dtype=float32), array(39642852., dtype=float32)]\n",
      "loss2 [array(135750.2, dtype=float32), array(132260., dtype=float32), array(125779.5, dtype=float32), array(128091.53, dtype=float32), array(127466.81, dtype=float32), array(117956.72, dtype=float32), array(122730.55, dtype=float32), array(127666.42, dtype=float32), array(122573.45, dtype=float32), array(129676.34, dtype=float32)]\n",
      "---> [Mean Time for Edge E1:\n",
      "ordinary   ---    54.3889\n",
      "connected  ---    57.7838\n",
      "All        ---    55.5413\n",
      ", Mean Fuel Consumption for Edge E1:\n",
      "ordinary   --- 91823.3887\n",
      "connected  --- 101038.8020\n",
      "All        --- 94951.5565\n",
      "]\n",
      " Retrying in 1 seconds\n",
      "noise_rate= 0.01\n",
      "global_step= 26553 Total reward in episode 52 = -656070.0733113686\n",
      "loss1 [array(1.308478e+08, dtype=float32), array(27103948., dtype=float32), array(83178632., dtype=float32), array(1.9692376e+08, dtype=float32), array(73836256., dtype=float32), array(1.5313802e+08, dtype=float32), array(56768280., dtype=float32), array(87660648., dtype=float32), array(22506210., dtype=float32), array(43964252., dtype=float32)]\n",
      "loss2 [array(92594.87, dtype=float32), array(97315.43, dtype=float32), array(83190.914, dtype=float32), array(86499.25, dtype=float32), array(86891.46, dtype=float32), array(88446.88, dtype=float32), array(74770.44, dtype=float32), array(78932.945, dtype=float32), array(70077.41, dtype=float32), array(76653.67, dtype=float32)]\n",
      "---> [Mean Time for Edge E1:\n",
      "ordinary   ---    54.3871\n",
      "connected  ---    57.1364\n",
      "All        ---    55.5283\n",
      ", Mean Fuel Consumption for Edge E1:\n",
      "ordinary   --- 93372.8796\n",
      "connected  --- 102922.4856\n",
      "All        --- 97336.8670\n",
      "]\n",
      " Retrying in 1 seconds\n",
      "noise_rate= 0.01\n",
      "global_step= 27054 Total reward in episode 53 = -574942.0893982726\n",
      "loss1 [array(2.59519e+08, dtype=float32), array(1.779217e+08, dtype=float32), array(1.3292378e+08, dtype=float32), array(40090440., dtype=float32), array(89629744., dtype=float32), array(60264564., dtype=float32), array(32884670., dtype=float32), array(39529088., dtype=float32), array(2.8807536e+08, dtype=float32), array(32065164., dtype=float32)]\n",
      "loss2 [array(121807.38, dtype=float32), array(122006.14, dtype=float32), array(118700.945, dtype=float32), array(119072.375, dtype=float32), array(120501.95, dtype=float32), array(125624.61, dtype=float32), array(130832.62, dtype=float32), array(135909.47, dtype=float32), array(134538.56, dtype=float32), array(134677.14, dtype=float32)]\n",
      "---> [Mean Time for Edge E1:\n",
      "ordinary   ---    54.3889\n",
      "connected  ---    57.7838\n",
      "All        ---    55.5413\n",
      ", Mean Fuel Consumption for Edge E1:\n",
      "ordinary   --- 91197.4328\n",
      "connected  --- 100439.5622\n",
      "All        --- 94334.6694\n",
      "]\n",
      " Retrying in 1 seconds\n",
      "noise_rate= 0.01\n",
      "global_step= 27555 Total reward in episode 54 = -685308.1279237175\n",
      "loss1 [array(1.0524386e+08, dtype=float32), array(23861172., dtype=float32), array(39977648., dtype=float32), array(46628824., dtype=float32), array(64691912., dtype=float32), array(48623040., dtype=float32), array(29946712., dtype=float32), array(32864670., dtype=float32), array(40439028., dtype=float32), array(74156648., dtype=float32)]\n",
      "loss2 [array(87338.04, dtype=float32), array(86176.09, dtype=float32), array(88193.89, dtype=float32), array(90274.305, dtype=float32), array(87089.336, dtype=float32), array(88593.64, dtype=float32), array(87885.22, dtype=float32), array(89399.94, dtype=float32), array(83530.59, dtype=float32), array(79578.39, dtype=float32)]\n",
      "---> [Mean Time for Edge E1:\n",
      "ordinary   ---    55.1067\n",
      "connected  ---    56.8056\n",
      "All        ---    55.6577\n",
      ", Mean Fuel Consumption for Edge E1:\n",
      "ordinary   --- 95070.4329\n",
      "connected  --- 108220.5400\n",
      "All        --- 99335.3325\n",
      "]\n",
      " Retrying in 1 seconds\n",
      "noise_rate= 0.01\n",
      "global_step= 28056 Total reward in episode 55 = -656251.378288677\n",
      "loss1 [array(1.4450443e+08, dtype=float32), array(49976984., dtype=float32), array(46216192., dtype=float32), array(80843984., dtype=float32), array(32408348., dtype=float32), array(46122964., dtype=float32), array(20331986., dtype=float32), array(82057144., dtype=float32), array(69194128., dtype=float32), array(18550482., dtype=float32)]\n",
      "loss2 [array(104125.16, dtype=float32), array(99874.11, dtype=float32), array(99968.69, dtype=float32), array(99236.766, dtype=float32), array(95126.16, dtype=float32), array(99106.66, dtype=float32), array(98251.55, dtype=float32), array(97754.35, dtype=float32), array(96626.14, dtype=float32), array(97429.09, dtype=float32)]\n",
      "---> [Mean Time for Edge E1:\n",
      "ordinary   ---    54.3871\n",
      "connected  ---    57.1364\n",
      "All        ---    55.5283\n",
      ", Mean Fuel Consumption for Edge E1:\n",
      "ordinary   --- 93372.8796\n",
      "connected  --- 102877.5764\n",
      "All        --- 97318.2254\n",
      "]\n",
      " Retrying in 1 seconds\n",
      "noise_rate= 0.01\n",
      "global_step= 28557 Total reward in episode 56 = -644048.8901737615\n",
      "loss1 [array(83380104., dtype=float32), array(26409914., dtype=float32), array(1.0135388e+08, dtype=float32), array(37996968., dtype=float32), array(52260676., dtype=float32), array(16381113., dtype=float32), array(57721196., dtype=float32), array(22683910., dtype=float32), array(12884520., dtype=float32), array(24816606., dtype=float32)]\n",
      "loss2 [array(90312.33, dtype=float32), array(80057., dtype=float32), array(93214.03, dtype=float32), array(89910.29, dtype=float32), array(89976.516, dtype=float32), array(87129.23, dtype=float32), array(84207.39, dtype=float32), array(84694.305, dtype=float32), array(91229.34, dtype=float32), array(87777.57, dtype=float32)]\n",
      "---> [Mean Time for Edge E1:\n",
      "ordinary   ---    55.1571\n",
      "connected  ---    56.4359\n",
      "All        ---    55.6147\n",
      ", Mean Fuel Consumption for Edge E1:\n",
      "ordinary   --- 95733.5491\n",
      "connected  --- 102186.5691\n",
      "All        --- 98042.4279\n",
      "]\n",
      " Retrying in 1 seconds\n",
      "noise_rate= 0.01\n",
      "global_step= 29058 Total reward in episode 57 = -642831.6844355837\n",
      "loss1 [array(77274504., dtype=float32), array(1.4063344e+08, dtype=float32), array(91137840., dtype=float32), array(2.0456635e+08, dtype=float32), array(1.4007342e+08, dtype=float32), array(2.1241296e+08, dtype=float32), array(90122384., dtype=float32), array(3.7272365e+08, dtype=float32), array(56940460., dtype=float32), array(66021384., dtype=float32)]\n",
      "loss2 [array(153719.64, dtype=float32), array(156459.02, dtype=float32), array(162261.75, dtype=float32), array(163760.31, dtype=float32), array(164303.69, dtype=float32), array(163888.83, dtype=float32), array(161480.1, dtype=float32), array(154569.06, dtype=float32), array(142745.42, dtype=float32), array(140118.81, dtype=float32)]\n",
      "---> [Mean Time for Edge E1:\n",
      "ordinary   ---    55.1429\n",
      "connected  ---    56.4359\n",
      "All        ---    55.6055\n",
      ", Mean Fuel Consumption for Edge E1:\n",
      "ordinary   --- 95887.0622\n",
      "connected  --- 101960.4514\n",
      "All        --- 98060.1097\n",
      "]\n",
      " Retrying in 1 seconds\n",
      "noise_rate= 0.01\n",
      "global_step= 29559 Total reward in episode 58 = -674292.6854195185\n",
      "loss1 [array(17945696., dtype=float32), array(31762680., dtype=float32), array(63922000., dtype=float32), array(23825986., dtype=float32), array(44543224., dtype=float32), array(33347804., dtype=float32), array(42654160., dtype=float32), array(37516944., dtype=float32), array(1.8164314e+08, dtype=float32), array(35321692., dtype=float32)]\n",
      "loss2 [array(114061.13, dtype=float32), array(114672.195, dtype=float32), array(114200.67, dtype=float32), array(114939.97, dtype=float32), array(113500.28, dtype=float32), array(116747.41, dtype=float32), array(119223.04, dtype=float32), array(115855.445, dtype=float32), array(118989.35, dtype=float32), array(121594.445, dtype=float32)]\n",
      "---> [Mean Time for Edge E1:\n",
      "ordinary   ---    54.8060\n",
      "connected  ---    57.7442\n",
      "All        ---    55.9545\n",
      ", Mean Fuel Consumption for Edge E1:\n",
      "ordinary   --- 91517.3162\n",
      "connected  --- 103914.5425\n",
      "All        --- 96363.5046\n",
      "]\n",
      " Retrying in 1 seconds\n",
      "noise_rate= 0.01\n",
      "global_step= 30060 Total reward in episode 59 = -642831.6844355837\n",
      "loss1 [array(1.0384852e+08, dtype=float32), array(57955252., dtype=float32), array(76401088., dtype=float32), array(39110656., dtype=float32), array(28876900., dtype=float32), array(87049104., dtype=float32), array(27424720., dtype=float32), array(60940436., dtype=float32), array(94629376., dtype=float32), array(29154014., dtype=float32)]\n",
      "loss2 [array(105523.5, dtype=float32), array(102563.45, dtype=float32), array(97983.33, dtype=float32), array(99152.64, dtype=float32), array(104321.86, dtype=float32), array(105265.766, dtype=float32), array(106718.08, dtype=float32), array(115483.06, dtype=float32), array(112819.375, dtype=float32), array(118144.484, dtype=float32)]\n",
      "---> [Mean Time for Edge E1:\n",
      "ordinary   ---    55.1429\n",
      "connected  ---    56.4359\n",
      "All        ---    55.6055\n",
      ", Mean Fuel Consumption for Edge E1:\n",
      "ordinary   --- 95887.0622\n",
      "connected  --- 101960.4514\n",
      "All        --- 98060.1097\n",
      "]\n",
      " Retrying in 1 seconds\n",
      "noise_rate= 0.01\n",
      "global_step= 30561 Total reward in episode 60 = -595850.9447823868\n",
      "loss1 [array(35728476., dtype=float32), array(51952268., dtype=float32), array(75555608., dtype=float32), array(82614232., dtype=float32), array(97232128., dtype=float32), array(53487668., dtype=float32), array(40778560., dtype=float32), array(72424400., dtype=float32), array(49427120., dtype=float32), array(42685672., dtype=float32)]\n",
      "loss2 [array(140984.66, dtype=float32), array(138704.84, dtype=float32), array(139503.44, dtype=float32), array(140691.08, dtype=float32), array(134119.62, dtype=float32), array(130655.34, dtype=float32), array(128590.92, dtype=float32), array(128495.59, dtype=float32), array(127585.445, dtype=float32), array(130010.31, dtype=float32)]\n",
      "---> [Mean Time for Edge E1:\n",
      "ordinary   ---    54.3889\n",
      "connected  ---    57.7838\n",
      "All        ---    55.5413\n",
      ", Mean Fuel Consumption for Edge E1:\n",
      "ordinary   --- 91823.3887\n",
      "connected  --- 101038.8020\n",
      "All        --- 94951.5565\n",
      "]\n",
      " Retrying in 1 seconds\n",
      "noise_rate= 0.01\n",
      "global_step= 31062 Total reward in episode 61 = -672977.79682234\n",
      "loss1 [array(45007768., dtype=float32), array(12067758., dtype=float32), array(28690520., dtype=float32), array(37539856., dtype=float32), array(22881192., dtype=float32), array(50179920., dtype=float32), array(24668158., dtype=float32), array(43924856., dtype=float32), array(46412176., dtype=float32), array(30079720., dtype=float32)]\n",
      "loss2 [array(59734.27, dtype=float32), array(60175.383, dtype=float32), array(60522.21, dtype=float32), array(67790.21, dtype=float32), array(64404.617, dtype=float32), array(65412.64, dtype=float32), array(71907.82, dtype=float32), array(78752.42, dtype=float32), array(72118.5, dtype=float32), array(74104.3, dtype=float32)]\n",
      "---> [Mean Time for Edge E1:\n",
      "ordinary   ---    55.0282\n",
      "connected  ---    57.2812\n",
      "All        ---    55.7282\n",
      ", Mean Fuel Consumption for Edge E1:\n",
      "ordinary   --- 91442.8123\n",
      "connected  --- 106880.1401\n",
      "All        --- 96238.8753\n",
      "]\n",
      " Retrying in 1 seconds\n",
      "noise_rate= 0.01\n",
      "global_step= 31563 Total reward in episode 62 = -625358.8440175666\n",
      "loss1 [array(49975176., dtype=float32), array(35641288., dtype=float32), array(35411004., dtype=float32), array(54231960., dtype=float32), array(2.0484096e+08, dtype=float32), array(82668240., dtype=float32), array(36782004., dtype=float32), array(38488544., dtype=float32), array(51028060., dtype=float32), array(1.0363102e+08, dtype=float32)]\n",
      "loss2 [array(100516.414, dtype=float32), array(117170.75, dtype=float32), array(114638.65, dtype=float32), array(111569.18, dtype=float32), array(124342.34, dtype=float32), array(120826.016, dtype=float32), array(122372.05, dtype=float32), array(121987.91, dtype=float32), array(126815.58, dtype=float32), array(125848.59, dtype=float32)]\n",
      "---> [Mean Time for Edge E1:\n",
      "ordinary   ---    55.0000\n",
      "connected  ---    56.6111\n",
      "All        ---    55.5524\n",
      ", Mean Fuel Consumption for Edge E1:\n",
      "ordinary   --- 92386.5043\n",
      "connected  --- 100234.8793\n",
      "All        --- 95077.3757\n",
      "]\n",
      " Retrying in 1 seconds\n",
      "noise_rate= 0.01\n",
      "global_step= 32064 Total reward in episode 63 = -656110.6764306586\n",
      "loss1 [array(74890448., dtype=float32), array(29737064., dtype=float32), array(55454344., dtype=float32), array(46426352., dtype=float32), array(74258176., dtype=float32), array(2.8333686e+08, dtype=float32), array(34433300., dtype=float32), array(1.01198984e+08, dtype=float32), array(85816176., dtype=float32), array(43205544., dtype=float32)]\n",
      "loss2 [array(141707.34, dtype=float32), array(139751.22, dtype=float32), array(145131.53, dtype=float32), array(137980.34, dtype=float32), array(138326.23, dtype=float32), array(143493.44, dtype=float32), array(147683.36, dtype=float32), array(149324.25, dtype=float32), array(142044.72, dtype=float32), array(136671.05, dtype=float32)]\n",
      "---> [Mean Time for Edge E1:\n",
      "ordinary   ---    54.3871\n",
      "connected  ---    57.1364\n",
      "All        ---    55.5283\n",
      ", Mean Fuel Consumption for Edge E1:\n",
      "ordinary   --- 93372.8796\n",
      "connected  --- 102883.8759\n",
      "All        --- 97320.8403\n",
      "]\n",
      " Retrying in 1 seconds\n",
      "noise_rate= 0.01\n",
      "global_step= 32565 Total reward in episode 64 = -625350.8484239659\n",
      "loss1 [array(27125638., dtype=float32), array(1.911569e+08, dtype=float32), array(22255822., dtype=float32), array(29746028., dtype=float32), array(26133942., dtype=float32), array(36109512., dtype=float32), array(25557024., dtype=float32), array(39954704., dtype=float32), array(38650340., dtype=float32), array(60067464., dtype=float32)]\n",
      "loss2 [array(87740.97, dtype=float32), array(87726.2, dtype=float32), array(92347.36, dtype=float32), array(90292.266, dtype=float32), array(94588.1, dtype=float32), array(96573.016, dtype=float32), array(102913.33, dtype=float32), array(107019.41, dtype=float32), array(109346.375, dtype=float32), array(111513.375, dtype=float32)]\n",
      "---> [Mean Time for Edge E1:\n",
      "ordinary   ---    55.0000\n",
      "connected  ---    56.6111\n",
      "All        ---    55.5524\n",
      ", Mean Fuel Consumption for Edge E1:\n",
      "ordinary   --- 92159.4283\n",
      "connected  --- 100283.1687\n",
      "All        --- 94944.7107\n",
      "]\n",
      " Retrying in 1 seconds\n",
      "noise_rate= 0.01\n",
      "global_step= 33066 Total reward in episode 65 = -661402.0900920549\n",
      "loss1 [array(45850536., dtype=float32), array(1.8098893e+08, dtype=float32), array(1.3712024e+08, dtype=float32), array(29694110., dtype=float32), array(89866192., dtype=float32), array(1.0331059e+08, dtype=float32), array(65520104., dtype=float32), array(19323118., dtype=float32), array(21725804., dtype=float32), array(49531264., dtype=float32)]\n",
      "loss2 [array(90517.16, dtype=float32), array(85363.19, dtype=float32), array(86661.266, dtype=float32), array(82824.78, dtype=float32), array(89221.92, dtype=float32), array(77581.09, dtype=float32), array(84767.61, dtype=float32), array(80372.47, dtype=float32), array(81722.58, dtype=float32), array(79511.05, dtype=float32)]\n",
      "---> [Mean Time for Edge E1:\n",
      "ordinary   ---    55.1774\n",
      "connected  ---    56.9535\n",
      "All        ---    55.9048\n",
      ", Mean Fuel Consumption for Edge E1:\n",
      "ordinary   --- 91734.7736\n",
      "connected  --- 101690.1301\n",
      "All        --- 95811.7291\n",
      "]\n",
      " Retrying in 1 seconds\n",
      "noise_rate= 0.01\n",
      "global_step= 33567 Total reward in episode 66 = -656210.406835969\n",
      "loss1 [array(24040596., dtype=float32), array(62107344., dtype=float32), array(2.0335677e+08, dtype=float32), array(40324916., dtype=float32), array(51329020., dtype=float32), array(94958640., dtype=float32), array(34595940., dtype=float32), array(1.4723312e+08, dtype=float32), array(46190488., dtype=float32), array(78430656., dtype=float32)]\n",
      "loss2 [array(101267.234, dtype=float32), array(102649.84, dtype=float32), array(100443.38, dtype=float32), array(102902.5, dtype=float32), array(104584.67, dtype=float32), array(102710.69, dtype=float32), array(105687.125, dtype=float32), array(110595.33, dtype=float32), array(103531.83, dtype=float32), array(103066.234, dtype=float32)]\n",
      "---> [Mean Time for Edge E1:\n",
      "ordinary   ---    54.3871\n",
      "connected  ---    57.1364\n",
      "All        ---    55.5283\n",
      ", Mean Fuel Consumption for Edge E1:\n",
      "ordinary   --- 93372.8796\n",
      "connected  --- 102877.5764\n",
      "All        --- 97318.2254\n",
      "]\n",
      " Retrying in 1 seconds\n",
      "noise_rate= 0.01\n",
      "global_step= 34068 Total reward in episode 67 = -594964.4683806777\n",
      "loss1 [array(46207124., dtype=float32), array(29381670., dtype=float32), array(48987276., dtype=float32), array(1.6923949e+08, dtype=float32), array(45896052., dtype=float32), array(22012744., dtype=float32), array(42150164., dtype=float32), array(39158224., dtype=float32), array(23808628., dtype=float32), array(28885228., dtype=float32)]\n",
      "loss2 [array(114823.016, dtype=float32), array(114212.96, dtype=float32), array(122665.5, dtype=float32), array(120540.37, dtype=float32), array(119125.51, dtype=float32), array(129288.62, dtype=float32), array(123798.086, dtype=float32), array(124725.5, dtype=float32), array(122630.56, dtype=float32), array(121389.234, dtype=float32)]\n",
      "---> [Mean Time for Edge E1:\n",
      "ordinary   ---    54.3889\n",
      "connected  ---    57.7838\n",
      "All        ---    55.5413\n",
      ", Mean Fuel Consumption for Edge E1:\n",
      "ordinary   --- 91823.4203\n",
      "connected  --- 101045.0890\n",
      "All        --- 94953.7115\n",
      "]\n",
      " Retrying in 1 seconds\n",
      "noise_rate= 0.01\n",
      "global_step= 34569 Total reward in episode 68 = -685308.1279237175\n",
      "loss1 [array(73537504., dtype=float32), array(47740944., dtype=float32), array(2.7633344e+08, dtype=float32), array(1.3354289e+08, dtype=float32), array(81236240., dtype=float32), array(58722108., dtype=float32), array(40533884., dtype=float32), array(1.578952e+08, dtype=float32), array(67332560., dtype=float32), array(3.5705357e+08, dtype=float32)]\n",
      "loss2 [array(128043.41, dtype=float32), array(129530.945, dtype=float32), array(132676.19, dtype=float32), array(129425.81, dtype=float32), array(131559.5, dtype=float32), array(135616.42, dtype=float32), array(141286.03, dtype=float32), array(146335.5, dtype=float32), array(145573.1, dtype=float32), array(143955.86, dtype=float32)]\n",
      "---> [Mean Time for Edge E1:\n",
      "ordinary   ---    55.1067\n",
      "connected  ---    56.8056\n",
      "All        ---    55.6577\n",
      ", Mean Fuel Consumption for Edge E1:\n",
      "ordinary   --- 95070.4329\n",
      "connected  --- 108220.5400\n",
      "All        --- 99335.3325\n",
      "]\n",
      " Retrying in 1 seconds\n",
      "noise_rate= 0.01\n",
      "global_step= 35070 Total reward in episode 69 = -595850.9447823868\n",
      "loss1 [array(1.0244416e+08, dtype=float32), array(41596524., dtype=float32), array(72298488., dtype=float32), array(30566914., dtype=float32), array(65004676., dtype=float32), array(81532544., dtype=float32), array(65128052., dtype=float32), array(38732864., dtype=float32), array(44225680., dtype=float32), array(80301336., dtype=float32)]\n",
      "loss2 [array(102054.56, dtype=float32), array(106607.875, dtype=float32), array(104405.72, dtype=float32), array(104411.85, dtype=float32), array(103874.22, dtype=float32), array(106440.33, dtype=float32), array(109238.125, dtype=float32), array(108399.8, dtype=float32), array(114148.125, dtype=float32), array(120064.3, dtype=float32)]\n",
      "---> [Mean Time for Edge E1:\n",
      "ordinary   ---    54.3889\n",
      "connected  ---    57.7838\n",
      "All        ---    55.5413\n",
      ", Mean Fuel Consumption for Edge E1:\n",
      "ordinary   --- 91823.3887\n",
      "connected  --- 101038.8020\n",
      "All        --- 94951.5565\n",
      "]\n",
      " Retrying in 1 seconds\n",
      "noise_rate= 0.01\n",
      "global_step= 35571 Total reward in episode 70 = -595850.9447823868\n",
      "loss1 [array(45224760., dtype=float32), array(45267336., dtype=float32), array(61064852., dtype=float32), array(39455252., dtype=float32), array(80359904., dtype=float32), array(65509664., dtype=float32), array(3.332185e+08, dtype=float32), array(97233864., dtype=float32), array(56713920., dtype=float32), array(49955792., dtype=float32)]\n",
      "loss2 [array(129196.83, dtype=float32), array(146724.31, dtype=float32), array(152085.27, dtype=float32), array(141326.94, dtype=float32), array(146035.36, dtype=float32), array(155317.7, dtype=float32), array(148605.06, dtype=float32), array(146847.47, dtype=float32), array(146694.38, dtype=float32), array(147900.62, dtype=float32)]\n",
      "---> [Mean Time for Edge E1:\n",
      "ordinary   ---    54.3889\n",
      "connected  ---    57.7838\n",
      "All        ---    55.5413\n",
      ", Mean Fuel Consumption for Edge E1:\n",
      "ordinary   --- 91823.3887\n",
      "connected  --- 101038.8020\n",
      "All        --- 94951.5565\n",
      "]\n",
      " Retrying in 1 seconds\n",
      "noise_rate= 0.01\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "#Основная функция программы\n",
    "def main():\n",
    "    #Загружаем среду Starcraft II, карту, сложность противника и расширенную  \n",
    "    #награду \n",
    "    buf = ReplayBuffer(max_size=10000)\n",
    "    env = initSimulation()\n",
    "    # env = StarCraft2Env(map_name=\"3ps1zgWallFOX\", reward_only_positive=False, reward_scale_rate=200, difficulty=\"1\")\n",
    "    #Получаем и выводим на печать информацию о среде\n",
    "    # env_info = env.get_env_info()\n",
    "    # print ('env_info=',env_info)\n",
    "    #Получаем и выводим на печать размер локальных состояний среды для агента\n",
    "    # obs_size =  env_info.get('obs_shape')\n",
    "    obs_size = 18\n",
    "    print (\"obs_size=\",obs_size)\n",
    "    #Количество действий агента \n",
    "    # n_actions = env_info[\"n_actions\"]\n",
    "    n_actions = 6\n",
    "    #Количество дружественных агентов\n",
    "    n_agents = 3\n",
    "    \n",
    "    #Определяем основные параметры нейросетевого обучения    \n",
    "    ##########################################################################\n",
    "    #Некоторые переходы в алгоритме MADDPG зависят от шагов игры\n",
    "    global_step = 0 #подсчитываем общее количество шагов в игре\n",
    "    start_steps = 1000 #начинаем обучать через 1000 шагов\n",
    "    steps_train = 10 #после начала обучения продолжаем обучать каждый 4 шаг \n",
    "    #Размер минивыборки \n",
    "    batch_size = 64\n",
    "    #Общее количество эпизодов игры\n",
    "    n_episodes = 510 \n",
    "    #Параметр дисконтирования.\n",
    "    gamma = 0.99 \n",
    "    #Скорость обучения исполнителя\n",
    "    alpha_actor = 0.01\n",
    "    #Скорость обучения критика\n",
    "    alpha_critic = 0.01 \n",
    "    #Уровень случайного шума\n",
    "    noise_rate = 0.01 \n",
    "    #Начальное значение случайного шума\n",
    "    noise_rate_max = 0.9\n",
    "    #Финальное значение случайного шума\n",
    "    noise_rate_min = 0.01 \n",
    "    #Шаг затухания уровня случайного шума\n",
    "    noise_decay_steps = 15000\n",
    "    #Параметр мягкой замены\n",
    "    tau = 0.01 \n",
    "    #Объем буфера воспроизведения\n",
    "    buffer_len = 10000\n",
    "    ###########################################################################   \n",
    "        \n",
    "    #Создаем буфер воспроизведения на основе deque\n",
    "    buf = ReplayBuffer(max_size=buffer_len)\n",
    "        \n",
    "    #Pytorch определяет возможность использования графического процессора\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    #Реализуем модифицированный алгоритм MADDPG \n",
    "    #с одной нейронной сетью критика и тремя нейронными сетями исполнителей  \n",
    "    #Создаем основную нейронную сеть исполнителя\n",
    "    actor_network = MADDPG_Actor(obs_size, n_actions).to(device)\n",
    "    #Создаем целевую нейронную сеть исполнителя\n",
    "    tgtActor_network = MADDPG_Actor(obs_size, n_actions).to(device)\n",
    "    #Синхронизуем веса нейронных сетей исполнителей\n",
    "    tgtActor_network.load_state_dict(actor_network.state_dict())\n",
    "    \n",
    "    #Создаем основную нейронную сеть критика\n",
    "    critic_network = MADDPG_Critic(obs_size*n_agents, n_agents).to(device)\n",
    "    #Создаем целевую нейронную сеть критика\n",
    "    tgtCritic_network = MADDPG_Critic(obs_size*n_agents, n_agents).to(device)\n",
    "    #Синхронизуем веса нейронных сетей критиков\n",
    "    tgtCritic_network.load_state_dict(critic_network.state_dict())\n",
    "    \n",
    "    #Создаем списки для мультиагентного случая\n",
    "    actor_network_list = []\n",
    "    tgtActor_network_list = []\n",
    "    optimizerActor_list = []\n",
    "    objectiveActor_list = []\n",
    "        \n",
    "    for agent_id in range(n_agents):\n",
    "        #Создаем список основных нейронных сетей исполнителей для трех агентов\n",
    "        actor_network_list.append(actor_network)\n",
    "        #Создаем список целевых нейронных сетей исполнителей\n",
    "        tgtActor_network_list.append(tgtActor_network)\n",
    "        #Создаем список оптимизаторов нейронных сетей исполнителей\n",
    "        optimizerActor_list.append(optim.Adam(params=actor_network_list[agent_id].parameters(), lr=alpha_actor))\n",
    "        #Создаем список функций потерь исполнителей\n",
    "        objectiveActor_list.append(nn.MSELoss())\n",
    "        \n",
    "    #Создаем оптимизатор нейронной сети критика\n",
    "    optimizerCritic = optim.Adam(params=critic_network.parameters(), lr=alpha_critic)\n",
    "    #Создаем функцию потерь критика\n",
    "    objectiveCritic = nn.MSELoss()\n",
    "    \n",
    "    #Выводим на печать архитектуру нейронных сетей\n",
    "    print ('Actor_network_list=', actor_network_list)\n",
    "    print ('Critic_network_list=', critic_network)\n",
    "            \n",
    "    #Определяем вспомогательные параметры\n",
    "    Loss_History = [] \n",
    "    Loss_History_actor = []\n",
    "    Reward_History = []\n",
    "    winrate_history = []\n",
    "    total_loss = []\n",
    "    total_loss_actor = []\n",
    "    m_loss = []\n",
    "    m_loss_actor = []\n",
    "    \n",
    "\n",
    "    #Основной цикл по эпизодам игры\n",
    "    ################_цикл for по эпизодам_#####################################\n",
    "    for e in range(n_episodes):\n",
    "       \n",
    "        #Перезагружаем среду\n",
    "        env.close()\n",
    "        env = initSimulation()\n",
    "        #Флаг окончания эпизода\n",
    "        terminated = False\n",
    "        #Награда за эпизод\n",
    "        episode_reward = 0\n",
    "        #Обновляем и выводим динамический уровень случайного шума\n",
    "        noise_rate = max(noise_rate_min, noise_rate_max - (noise_rate_max-noise_rate_min) * global_step/noise_decay_steps)\n",
    "        print ('noise_rate=', noise_rate)\n",
    "                \n",
    "        #Шаги игры внутри эпизода\n",
    "        ######################_цикл while_#####################################\n",
    "        while not terminated:\n",
    "            #Обнуляем промежуточные переменные\n",
    "            actions = []\n",
    "            observations = []\n",
    "            action = 0\n",
    "            #Храним историю действий один шаг для разных агентов\n",
    "            actionsFox = np.zeros([n_agents]) \n",
    "            #Храним историю состояний среды один шаг для разных агентов\n",
    "            obs_agent = np.zeros([n_agents], dtype=object) \n",
    "            obs_agent_next = np.zeros([n_agents], dtype=object)\n",
    "            \n",
    "            done = False\n",
    "\n",
    "            ###########_Цикл по агентам для выполнения действий в игре_########\n",
    "            for agent_id in range(n_agents):\n",
    "                #Получаем состояние среды для независимого агента \n",
    "                obs_agent[agent_id] = env.get_obs_agent(agent_id)\n",
    "                # if tuple(obs_agent[agent_id]) == tuple(DEFAULT_STATE):\n",
    "                #     done = True\n",
    "\n",
    "                #Конвертируем данные в тензор\n",
    "                obs_agentT = torch.FloatTensor([obs_agent[agent_id]]).to(device)\n",
    "                #Передаем состояние среды в основную нейронную сеть \n",
    "                #и получаем стратегию действий\n",
    "                action_probabilitiesT = actor_network_list[agent_id](obs_agentT)\n",
    "                #Конвертируем данные в numpy\n",
    "                action_probabilitiesT = action_probabilitiesT.to(\"cpu\")\n",
    "                action_probabilities = action_probabilitiesT.data.numpy()[0]\n",
    "                \n",
    "                #Находим возможные действия агента в данный момент времени \n",
    "                avail_actions = env.get_avail_agent_actions(agent_id)\n",
    "                avail_actions_ind = np.nonzero(avail_actions)[0]\n",
    "                #Выбираем возможное действие агента с учетом\n",
    "                #стратегии действий и уровня случайного шума\n",
    "                action = select_actionFox(action_probabilities, avail_actions_ind, n_actions, noise_rate)\n",
    "                #Обрабатываем исключение при ошибке в возможных действиях\n",
    "                if action is None:\n",
    "                    action = np.random.choice (avail_actions_ind)\n",
    "                    \n",
    "                #Собираем действия от разных агентов               \n",
    "                actions.append(action)\n",
    "                actionsFox[agent_id] = action\n",
    "                #Собираем локальные состояния среды от разных агентов\n",
    "                for i in range(obs_size):\n",
    "                    observations.append(obs_agent[agent_id][i])\n",
    "            ######_конец цикла по агентам для выполнения действий в игре_######\n",
    "\n",
    "            #Передаем действия агентов в среду, получаем награду\n",
    "            #и прерывание игры от среды\n",
    "            reward, terminated = env.step(actions)\n",
    "            #Суммируем награды за этот шаг для вычисления награды за эпизод\n",
    "            episode_reward += reward\n",
    "            \n",
    "            #Подготовляем данные для сохранения в буфере воспроизведения\n",
    "            actions_next = []\n",
    "            observations_next = []\n",
    "            #Если эпизод не завершился, то можно найти новые действия и состояния\n",
    "            if terminated == False:\n",
    "                for agent_id in range(n_agents):\n",
    "                    #Получаем новое состояние среды для независимого агента \n",
    "                    obs_agent_next[agent_id] = env.get_obs_agent(agent_id)\n",
    "\n",
    "                    # if tuple(obs_agent_next[agent_id]) == tuple(DEFAULT_STATE):\n",
    "                    #     done = True\n",
    "                    #Собираем от разных агентов новые состояния\n",
    "                    for i in range(obs_size):\n",
    "                        observations_next.append(obs_agent_next[agent_id][i])\n",
    "                    #Конвертируем данные в тензор\n",
    "                    obs_agent_nextT = torch.FloatTensor([obs_agent_next[agent_id]]).to(device)\n",
    "                    #Получаем новые действия агентов для новых состояний\n",
    "                    #из целевой сети исполнителя\n",
    "                    action_probabilitiesT = tgtActor_network_list[agent_id](obs_agent_nextT)\n",
    "                    #Конвертируем данные в numpy\n",
    "                    action_probabilitiesT = action_probabilitiesT.to(\"cpu\")\n",
    "                    action_probabilities = action_probabilitiesT.data.numpy()[0]\n",
    "                    #Находим новые возможные действия агента\n",
    "                    avail_actions = env.get_avail_agent_actions(agent_id)\n",
    "                    avail_actions_ind = np.nonzero(avail_actions)[0]\n",
    "                    #Выбираем новые возможные действия\n",
    "                    action = select_actionFox(action_probabilities, avail_actions_ind, n_actions, noise_rate)\n",
    "                    if action == 0:\n",
    "                        pass\n",
    "                    if action is None:\n",
    "                        action = np.random.choice (avail_actions_ind)\n",
    "                    #Собираем новые действия от разных агентов\n",
    "\n",
    "                    actions_next.append(action)\n",
    "            elif terminated == True:\n",
    "                #если эпизод на этом шаге завершился, то новых действий не будет\n",
    "                actions_next = actions \n",
    "\n",
    "                observations_next = observations\n",
    "            \n",
    "            #Сохраняем переход в буфере воспроизведения \n",
    "            if not done:\n",
    "                pass\n",
    "            buf.put((observations, actions, observations_next, actions_next, reward, terminated or done))\n",
    "            \n",
    "            #Если буфер воспроизведения наполнен, начинаем обучать сеть\n",
    "            ########################_начало if обучения_#######################\n",
    "            if (global_step % steps_train == 0) and (global_step > start_steps):\n",
    "                #Получаем минивыборку из буфера воспроизведения\n",
    "                exp_obs, exp_acts, exp_next_obs, exp_next_acts, exp_rew, exp_termd = buf.sample(batch_size)\n",
    "                    \n",
    "                #Конвертируем данные в тензор\n",
    "                exp_obs = [np.array(x) for x in exp_obs]\n",
    "                obs_agentsT = torch.FloatTensor([exp_obs]).to(device)\n",
    "                exp_acts = [np.array(x) for x in exp_acts]\n",
    "                act_agentsT = torch.FloatTensor([exp_acts]).to(device)\n",
    "                                    \n",
    "                ###############_Обучаем нейронную сеть критика_################\n",
    "                \n",
    "                #Получаем значения из основной сети критика\n",
    "                action_probabilitieQT = critic_network(obs_agentsT, act_agentsT)\n",
    "                action_probabilitieQT = action_probabilitieQT.to(\"cpu\")\n",
    "                               \n",
    "                #Конвертируем данные в тензор\n",
    "                exp_next_obs = [np.array(x) for x in exp_next_obs]\n",
    "                obs_agents_nextT = torch.FloatTensor([exp_next_obs]).to(device)\n",
    "                exp_next_acts = [np.array(x) for x in exp_next_acts]\n",
    "                act_agents_nextT = torch.FloatTensor([exp_next_acts]).to(device)\n",
    "                                        \n",
    "                #Получаем значения из целевой сети критика\n",
    "                action_probabilitieQ_nextT = tgtCritic_network(obs_agents_nextT, act_agents_nextT)\n",
    "                action_probabilitieQ_nextT = action_probabilitieQ_nextT.to(\"cpu\")\n",
    "                action_probabilitieQ_next = action_probabilitieQ_nextT.data.numpy()[0]\n",
    "                    \n",
    "                #Переформатируем y_batch размером batch_size\n",
    "                y_batch = np.zeros([batch_size])\n",
    "                action_probabilitieQBT = torch.empty(1, batch_size, dtype=torch.float)\n",
    "                \n",
    "                for i in range (batch_size):\n",
    "                    #Вычисляем целевое значение y \n",
    "                    y_batch[i] = exp_rew[i] + (gamma*action_probabilitieQ_next[i])*(1 - np.array(exp_termd[i]))\n",
    "                    action_probabilitieQBT[0][i] = action_probabilitieQT[0][i]\n",
    "                \n",
    "                y_batchT = torch.FloatTensor([y_batch])\n",
    "                \n",
    "                #Обнуляем градиенты\n",
    "                optimizerCritic.zero_grad()\n",
    "                 \n",
    "                #Вычисляем функцию потерь критика\n",
    "                loss_t_critic = objectiveCritic(action_probabilitieQBT, y_batchT) \n",
    "                    \n",
    "                #Сохраняем данные для графиков\n",
    "                Loss_History.append(loss_t_critic) \n",
    "                loss_n_critic = loss_t_critic.data.numpy()\n",
    "                total_loss.append(loss_n_critic)\n",
    "                m_loss.append(np.mean(total_loss[-1000:]))\n",
    "                    \n",
    "                #Выполняем обратное распространение ошибки для критика\n",
    "                loss_t_critic.backward()\n",
    "                \n",
    "                #Выполняем оптимизацию нейронной сети критика\n",
    "                optimizerCritic.step()\n",
    "                ###################_Закончили обучать критика_#################\n",
    "                \n",
    "                ##############_Обучаем нейронные сети исполнителей_############\n",
    "                #Разбираем совместное состояние на локальные состояния\n",
    "\n",
    "                act_full = np.zeros([batch_size, n_agents])\n",
    "\n",
    "                for z in range(n_agents):\n",
    "                    obs_localz = np.zeros([batch_size, obs_size])\n",
    "                    for i in range (batch_size):\n",
    "                        k = 0\n",
    "                        for j in range (obs_size*z, obs_size*(z + 1)):\n",
    "                            obs_localz[i][k] = exp_obs[i][j]\n",
    "                            k += 1\n",
    "\n",
    "                    obs_agentTz = torch.FloatTensor([obs_localz]).to(device)\n",
    "                    optimizerActor_list[z].zero_grad()\n",
    "                    action_probabilitiesT1 = actor_network_list[z](obs_agentTz)\n",
    "                    action_probabilitiesT1 = action_probabilitiesT1.to(\"cpu\")\n",
    "                    action_probabilities1 = action_probabilitiesT1.data.numpy()[0]\n",
    "\n",
    "                    for i in range (batch_size):\n",
    "                        act_full[i][z] = np.argmax(action_probabilities1[i])\n",
    "\n",
    "                act_fullT = torch.FloatTensor([act_full]).to(device)\n",
    "\n",
    "\n",
    "                \n",
    "                #Конвертируем данные в тензор\n",
    "                exp_obs = [x for x in exp_obs]\n",
    "                obs_agentsT = torch.FloatTensor([exp_obs]).to(device)\n",
    "                                \n",
    "                #Задаем значение функции потерь для нерйонных сетей исполнителей\n",
    "                #как отрицательный выход критика\n",
    "                actor_lossT = -critic_network(obs_agentsT, act_fullT)\n",
    "                \n",
    "                #Усредняем значение по количеству элементов минивыборки\n",
    "                actor_lossT = actor_lossT.mean()    \n",
    "                \n",
    "                #Выполняем обратное распространение ошибки\n",
    "                actor_lossT.backward()\n",
    "                \n",
    "                #Выполняем оптимизацию нейронных сетей исполнителей\n",
    "                for z in range(n_agents):\n",
    "                    optimizerActor_list[z].step()\n",
    "                \n",
    "                #Собираем данные для графиков\n",
    "                actor_lossT = actor_lossT.to(\"cpu\")\n",
    "                Loss_History_actor.append(actor_lossT) \n",
    "                actor_lossN = actor_lossT.data.numpy()\n",
    "                total_loss_actor.append(actor_lossN)\n",
    "                m_loss_actor.append(np.mean(total_loss_actor[-1000:]))\n",
    "    \n",
    "                ##############_Закончили обучать исполнителей_#################\n",
    "                \n",
    "                #Рализуем механизм мягкой замены\n",
    "                #Обновляем целевую сеть критика\n",
    "                for target_param, param in zip(tgtCritic_network.parameters(), critic_network.parameters()):\n",
    "                    target_param.data.copy_((1 - tau) * param.data + tau * target_param.data)\n",
    "                #Обновляем целевые сети акторов\n",
    "                for agent_id in range(n_agents):\n",
    "                    for target_param, param in zip(tgtActor_network_list[agent_id].parameters(), actor_network_list[agent_id].parameters()):\n",
    "                        target_param.data.copy_((1 - tau) * param.data + tau * target_param.data)\n",
    " \n",
    "                ######################_конец if обучения_######################\n",
    "                \n",
    "            #Обновляем счетчик общего количества шагов\n",
    "            global_step += 1\n",
    "        \n",
    "        ######################_конец цикла while_##############################\n",
    "        writer.add_scalar('reward/train', episode_reward , e)\n",
    "        #Выводим на печать счетчик шагов игры и общую награду за эпизод\n",
    "        print('global_step=', global_step, \"Total reward in episode {} = {}\".format(e, episode_reward))\n",
    "        print(\"loss1\", total_loss[-10:])\n",
    "        print(\"loss2\", total_loss_actor[-10:])\n",
    "        \n",
    "        #Собираем данные для графиков\n",
    "        Reward_History.append(episode_reward)\n",
    "        # status = env.get_stats()\n",
    "        # winrate_history.append(status[\"win_rate\"])\n",
    "        \n",
    "    ################_конец цикла по эпизодам игры_#############################\n",
    "    \n",
    "    #Закрываем среду StarCraft II\n",
    "    env.close()\n",
    "    \n",
    "    #Сохраняем параметры обученных нейронных сетей\n",
    "    for agent_id in range(n_agents):\n",
    "        torch.save(actor_network_list[agent_id].state_dict(),\"actornet_%.0f.dat\"%agent_id) \n",
    "    \n",
    "    #Выводим на печать графики\n",
    "    #Средняя награда\n",
    "    plt.figure(num=None, figsize=(6, 3), dpi=150, facecolor='w', edgecolor='k')\n",
    "    plt.plot(Reward_History)\n",
    "    plt.xlabel('Номер эпизода')\n",
    "    plt.ylabel('Количество награды за эпизод')\n",
    "    plt.show()\n",
    "    #Процент побед\n",
    "    plt.figure(num=None, figsize=(6, 3), dpi=150, facecolor='w', edgecolor='k')\n",
    "    plt.plot(winrate_history)\n",
    "    plt.xlabel('Номер эпизода')\n",
    "    plt.ylabel('Процент побед')\n",
    "    plt.show()\n",
    "    #Значения функции потерь исполнителя\n",
    "    plt.figure(num=None, figsize=(6, 3), dpi=150, facecolor='w', edgecolor='k')\n",
    "    plt.plot(m_loss_actor)\n",
    "    plt.xlabel('Номер каждой 1000 итерации')\n",
    "    plt.ylabel('Функция потерь исполнителя')\n",
    "    plt.show()\n",
    "    #Значения функции потерь критика\n",
    "    plt.figure(num=None, figsize=(6, 3), dpi=150, facecolor='w', edgecolor='k')\n",
    "    plt.plot(m_loss)\n",
    "    plt.xlabel('Номер каждой 1000 итерации')\n",
    "    plt.ylabel('Функция потерь критика')\n",
    "    plt.show()\n",
    "   \n",
    "#Точка входа в программу  \n",
    "if __name__ == \"__main__\":\n",
    "    start_time = time.time()\n",
    "    main() \n",
    "    print(\"--- %s минут ---\" % ((time.time() - start_time)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43menv\u001b[49m\u001b[38;5;241m.\u001b[39mclose()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'env' is not defined"
     ]
    }
   ],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'chess'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpettingzoo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclassic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m chess_v6\n\u001b[1;32m      3\u001b[0m env \u001b[38;5;241m=\u001b[39m chess_v6\u001b[38;5;241m.\u001b[39menv(render_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m env\u001b[38;5;241m.\u001b[39mreset(seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pettingzoo/classic/__init__.py:5\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(env_name)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(env_name):\n\u001b[0;32m----> 5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdeprecated_handler\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m__path__\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pettingzoo/utils/deprecated_module.py:60\u001b[0m, in \u001b[0;36mdeprecated_handler\u001b[0;34m(env_name, module_path, module_name)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# This executes the module and will raise any exceptions\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# that would typically be raised by just `import blah`\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m spec\u001b[38;5;241m.\u001b[39mloader\n\u001b[0;32m---> 60\u001b[0m \u001b[43mspec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexec_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pettingzoo/classic/chess_v6.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpettingzoo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclassic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchess\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchess\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m env, raw_env\n\u001b[1;32m      3\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menv\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_env\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pettingzoo/classic/chess/chess.py:109\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m__future__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m annotations\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m path\n\u001b[0;32m--> 109\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mchess\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgymnasium\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'chess'"
     ]
    }
   ],
   "source": [
    "from pettingzoo.classic import chess_v6\n",
    "\n",
    "env = chess_v6.env(render_mode=\"human\")\n",
    "env.reset(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43menv\u001b[49m\u001b[38;5;241m.\u001b[39maction_spaces\n",
      "\u001b[0;31mNameError\u001b[0m: name 'env' is not defined"
     ]
    }
   ],
   "source": [
    "env.action_spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting chess\n",
      "  Downloading chess-1.10.0-py3-none-any.whl (154 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.4/154.4 KB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: chess\n",
      "Successfully installed chess-1.10.0\n"
     ]
    }
   ],
   "source": [
    "!pip install chess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method BaseWrapper.action_space of <pettingzoo.utils.wrappers.assert_out_of_bounds.AssertOutOfBoundsWrapper object at 0x7fe19c12ff40>>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
