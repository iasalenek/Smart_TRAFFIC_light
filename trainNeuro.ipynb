{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "import torch as th\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from torch import optim\n",
    "from src.model.replayBuffer import ReplayBuffer\n",
    "# from simulation import *\n",
    "from src.simulationUtils.simulation import *\n",
    "from src.simulationUtils import *\n",
    "\n",
    "device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(10,100)\n",
    "        self.fc5 = nn.Linear(100,5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc5(x)\n",
    "        return x\n",
    "\n",
    "    def sample_action(self, obs, epsilon):\n",
    "        out = self.forward(obs)\n",
    "        coin = random.random()\n",
    "        if coin < epsilon:\n",
    "            return random.randint(0, 4)\n",
    "        else:\n",
    "            return out.argmax().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(q, q_target, replay_buffer, optimizer, batch_size, gamma, updates_number=10):\n",
    "    for _ in range(updates_number):\n",
    "\n",
    "        s, a, r, s_prime, done_mask = replay_buffer.sample(batch_size)\n",
    "\n",
    "        # полезность\n",
    "        q_out = q(s)\n",
    "        a = a.unsqueeze(1)\n",
    "        q_a = q_out.gather(1, a)\n",
    "        max_q_prime = q_target(s_prime).max(1)[0].unsqueeze(1)\n",
    "        target = r.unsqueeze(1) +  gamma * max_q_prime * done_mask.unsqueeze(1)\n",
    "\n",
    "        loss = F.smooth_l1_loss(q_a, target.detach())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = list()\n",
    "total = list()\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "# DEFAULT_STATE = (0,0,0,0,0,0,0,0,0,0)\n",
    "\n",
    "def run(learning_rate, gamma, buffer_max_size, batch_size, target_update_interval,\n",
    "        replay_buffer_start_size, print_interval=20, n_episodes=10000):\n",
    "\n",
    "    q = QNetwork()\n",
    "    q_target = QNetwork()\n",
    "\n",
    "    q_target.load_state_dict(q.state_dict())\n",
    "\n",
    "    replay_buffer = ReplayBuffer(max_size=buffer_max_size)\n",
    "\n",
    "    score = 0.0\n",
    "\n",
    "    optimizer = optim.Adam(q.parameters(), lr=learning_rate)\n",
    "\n",
    "    for n_epi in range(n_episodes):\n",
    "        epsilon = max(0.01, 0.08 - 0.01 * (n_epi / 200))\n",
    "        env = initSimulation()\n",
    "        s, _, _ = env.step({0:0})\n",
    "        s = np.array(s)\n",
    "        s = s.reshape(-1)\n",
    "        \n",
    "        for g in range(300):\n",
    "            s = np.array(s)\n",
    "            a = q.sample_action(th.from_numpy(s).float(), epsilon)\n",
    "            # s_prime, r,  terminated, truncated, _ = env.step(a)\n",
    "            s_prime, r, _ = env.step({0:a})\n",
    "\n",
    "            done_mask = 0.0  if (tuple(s_prime) == DEFAULT_STATE or tuple(s) == DEFAULT_STATE) else 1.0\n",
    "            replay_buffer.put((s, a, r/100.0, s_prime, done_mask))\n",
    "            s = s_prime\n",
    "            score += r\n",
    "            if g % 10 == 0 and len(replay_buffer) > replay_buffer_start_size:\n",
    "                train(q, q_target, replay_buffer, optimizer, batch_size, gamma)\n",
    "\n",
    "\n",
    "            # if truncated or terminated:\n",
    "            #     break\n",
    "        if n_epi % 20:\n",
    "            print(step)\n",
    "            print(total)\n",
    "        # writer.add_scalar('reward/train', score , n_epi)\n",
    "        env.close()\n",
    "        step.append(n_epi)\n",
    "        total.append(score)\n",
    "        print(len(replay_buffer))\n",
    "        # if len(replay_buffer) > replay_buffer_start_size:\n",
    "        #     train(q, q_target, replay_buffer, optimizer, batch_size, gamma)\n",
    "\n",
    "\n",
    "        if n_epi % target_update_interval == 0 and n_epi != 0:\n",
    "            q_target.load_state_dict(q.state_dict())\n",
    "        print(\"# of episode :{}, abg score : {:.1f}, buffer size : {}, epsilon : {:.1f}%\"\n",
    "                .format(n_epi, score/ print_interval, len(replay_buffer), epsilon * 100))\n",
    "        score = 0.0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run(learning_rate=0.001,\n",
    "#     gamma=0.98,\n",
    "#     buffer_max_size=10000,\n",
    "#     batch_size=64,\n",
    "#     target_update_interval=10,\n",
    "#     replay_buffer_start_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.simulationUtils.simulation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.simulationUtils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.simulationUtils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pettingzoo.butterfly import knights_archers_zombies_v10\n",
    "# env = knights_archers_zombies_v10.env(render_mode=\"human\")\n",
    "# env.reset(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = initSimulation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.from_id_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(100):\n",
    "#     env.step({0:0,1:0,2:0,3:0,4:0,5:0})\n",
    "#     print(env.use_real_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.step(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(1000):\n",
    "#     v = env.step(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Мультиагентное обучение с подкреплением\n",
    "#Глава 3. Нейросетевое обучение\n",
    "#Алгоритм MADDPG\n",
    "\n",
    "#Подключаем библиотеки\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "#Флаг вывода массива целиком\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "\n",
    "#Определяем архитектуру нейронной сети исполнителя\n",
    "class MADDPG_Actor(nn.Module):\n",
    "    def __init__(self, obs_size, n_actions):\n",
    "        super(MADDPG_Actor, self).__init__()\n",
    "        #На вход нейронная сеть получает состояние среды для отдельного агента \n",
    "        #На выходе нейронная сеть возвращает стратегию действий\n",
    "        self.MADDPG_Actor = nn.Sequential(\n",
    "            #Первый линейный слой обрабатывает входные данные состояния среды\n",
    "            nn.Linear(obs_size, 60),\n",
    "            nn.ReLU(),\n",
    "            #Второй линейный слой обрабатывает внутренние данные \n",
    "            nn.Linear(60, 60),\n",
    "            nn.ReLU(),\n",
    "            #Третий линейный слой обрабатывает внутренние данные \n",
    "            nn.Linear(60, 60),\n",
    "            nn.ReLU(),\n",
    "            #Четвертый линейный слой обрабатывает данные для стратегии действий\n",
    "            nn.Linear(60, n_actions)\n",
    "            )\n",
    "        #Финальный выход нерйонной сети обрабатывается функцией Tanh()\n",
    "        self.tanh_layer = nn.Tanh()\n",
    "    #Вначале данные x обрабатываются полносвязной сетью с функцией ReLU\n",
    "    #На выходе происходит обработка функцией Tanh()\n",
    "    def forward(self, x):\n",
    "        #Обработка полносвязными линейными слоями\n",
    "        network_out = self.MADDPG_Actor(x)\n",
    "        #Обработка функцией Tanh()\n",
    "        tanh_layer_out = self.tanh_layer(network_out)\n",
    "        #Выход нейронной сети\n",
    "        return tanh_layer_out\n",
    "\n",
    "#Определяем архитектуру нейронной сети критика\n",
    "class MADDPG_Critic(nn.Module):\n",
    "    def __init__(self, full_obs_size, n_actions_agents):\n",
    "        super(MADDPG_Critic, self).__init__()\n",
    "        #На вход нейронная сеть получает состояние среды,\n",
    "        #включающее все локальные состояния среды от отдельных агентов\n",
    "        #и все выполненные действия отдельных агентов\n",
    "        #На выходе нейронная сеть возвращает корректирующее значение\n",
    "        self.network = nn.Sequential(\n",
    "            #Первый линейный слой обрабатывает входные данные    \n",
    "            nn.Linear(full_obs_size+n_actions_agents, 202),\n",
    "            nn.ReLU(),\n",
    "            #Второй линейный слой обрабатывает внутренние данные\n",
    "            nn.Linear(202, 60),\n",
    "            nn.ReLU(),\n",
    "            #Третий линейный слой обрабатывает внутренние данные\n",
    "            nn.Linear(60, 30),\n",
    "            nn.ReLU(),\n",
    "            #Четвертый линейный слой обрабатывает выходные данные\n",
    "            nn.Linear(30, 1)\n",
    "            )\n",
    "    #Данные x последовательно обрабатываются полносвязной сетью с функцией ReLU\n",
    "    def forward(self, state, action):\n",
    "        #Объединяем данные состояний и действий для передачи в сеть\n",
    "        x = torch.cat([state, action], dim=2)\n",
    "        #Результаты обработки \n",
    "        Q_value = self.network(x)\n",
    "        #Финальный выход нейронной сети\n",
    "        return Q_value\n",
    "    \n",
    "\n",
    "        \n",
    "#Выбираем возможное действие с максимальным из стратегии действий\n",
    "#с учетом дополнительного случайного шума\n",
    "def select_actionFox(act_prob, avail_actions_ind, n_actions, noise_rate):\n",
    "    p = np.random.random(1).squeeze()\n",
    "    #Добавляем случайный шум к действиям для исследования\n",
    "    #разных вариантов действий\n",
    "    for i in range(n_actions):\n",
    "        #Создаем шум заданного уровня\n",
    "        noise = noise_rate*(np.random.rand())\n",
    "        #Добавляем значение шума к значению вероятности выполнения действия\n",
    "        act_prob [i] =  act_prob [i] + noise\n",
    "    #Выбираем действия в зависимости от вероятностей их выполнения\n",
    "    for j in range(n_actions):\n",
    "        #Выбираем случайный элемент из списка \n",
    "        actiontemp =  random.choices([str(i) for i in range(n_actions)], weights=softmax(act_prob))\n",
    "        #Преобразуем тип данных\n",
    "        action = int (actiontemp[0])\n",
    "        #Проверяем наличие выбранного действия в списке действий\n",
    "        if action in avail_actions_ind:\n",
    "            return action\n",
    "        else:\n",
    "            act_prob[action] = 0\n",
    "          \n",
    "#Создаем минивыборку определенного объема из буфера воспроизведения        \n",
    "def sample_from_expbuf(experience_buffer, batch_size):\n",
    "    #Функция возвращает случайную последовательность заданной длины\n",
    "    perm_batch = np.random.permutation(len(experience_buffer))[:batch_size]\n",
    "    #Минивыборка\n",
    "    experience = np.array(experience_buffer)[perm_batch]\n",
    "    #Возвращаем значения минивыборки по частям\n",
    "    return experience[:,0], experience[:,1], experience[:,2], experience[:,3], experience[:,4], experience[:,5]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFAULT_STATE = (0,0,0,0,0,0,0,0,0,0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "TraCIException",
     "evalue": "Connection 'default' is already active.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTraCIException\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 457\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    456\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 457\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m    458\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--- \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m минут ---\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m ((time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m60\u001b[39m))\n",
      "Cell \u001b[0;32mIn[60], line 11\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m():\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m#Загружаем среду Starcraft II, карту, сложность противника и расширенную  \u001b[39;00m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m#награду \u001b[39;00m\n\u001b[1;32m     10\u001b[0m     buf \u001b[38;5;241m=\u001b[39m ReplayBuffer(max_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m     env \u001b[38;5;241m=\u001b[39m \u001b[43minitSimulation\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# env = StarCraft2Env(map_name=\"3ps1zgWallFOX\", reward_only_positive=False, reward_scale_rate=200, difficulty=\"1\")\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m#Получаем и выводим на печать информацию о среде\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# env_info = env.get_env_info()\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# print ('env_info=',env_info)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m#Получаем и выводим на печать размер локальных состояний среды для агента\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# obs_size =  env_info.get('obs_shape')\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     obs_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m18\u001b[39m\n",
      "File \u001b[0;32m~/RL/Smart_TRAFFIC_light/src/simulationUtils/simulation.py:226\u001b[0m, in \u001b[0;36minitSimulation\u001b[0;34m()\u001b[0m\n\u001b[1;32m    223\u001b[0m tf \u001b[38;5;241m=\u001b[39m trainTraffic(cm)\n\u001b[1;32m    224\u001b[0m cm\u001b[38;5;241m.\u001b[39msetTrainTraffic(tf)\n\u001b[0;32m--> 226\u001b[0m \u001b[43mcm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpolicyListner\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNeuroPolicy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpolicyOptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mspeed\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m}\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m tf\u001b[38;5;241m.\u001b[39mstep_while()\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tf\n",
      "File \u001b[0;32m~/RL/Smart_TRAFFIC_light/src/simulationUtils/simulation.py:157\u001b[0m, in \u001b[0;36mSimulationTraffic.start\u001b[0;34m(self, configPath, simTime, policyListner, policyOptions, pVehicle, pConnected, minSpeed, maxSpeed, stepLength, edgeIDs, trafficlightIDs, vehicletypeIDs, randomSeed, sumoSeed, useGUI)\u001b[0m\n\u001b[1;32m    145\u001b[0m     sumoBinary \u001b[38;5;241m=\u001b[39m sumolib\u001b[38;5;241m.\u001b[39mcheckBinary(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msumo\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    147\u001b[0m sumoCmd \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    148\u001b[0m     sumoBinary,\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-c\u001b[39m\u001b[38;5;124m\"\u001b[39m, configPath,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--time-to-teleport\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-1\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# Телепортация автомобилей отключена\u001b[39;00m\n\u001b[1;32m    155\u001b[0m ]\n\u001b[0;32m--> 157\u001b[0m \u001b[43mtraci\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43msumoCmd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m policyListner \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    160\u001b[0m     policyListner \u001b[38;5;241m=\u001b[39m policyListner(\n\u001b[1;32m    161\u001b[0m         edgeIDs\u001b[38;5;241m=\u001b[39medgeIDs,\n\u001b[1;32m    162\u001b[0m         trafficlightIDs\u001b[38;5;241m=\u001b[39mtrafficlightIDs,\n\u001b[1;32m    163\u001b[0m         model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainTraffic_,\n\u001b[1;32m    164\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpolicyOptions,\n\u001b[1;32m    165\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/traci/main.py:139\u001b[0m, in \u001b[0;36mstart\u001b[0;34m(cmd, port, numRetries, label, verbose, traceFile, traceGetters, stdout, doSwitch)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;124;03mStart a sumo server using cmd, establish a connection to it and\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;124;03mstore it under the given label. This method is not thread-safe.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;124;03m- stdout (iostream): where to pipe sumo process stdout\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection\u001b[38;5;241m.\u001b[39mhas(label):\n\u001b[0;32m--> 139\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m TraCIException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is already active.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m label)\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m numRetries \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m connection\u001b[38;5;241m.\u001b[39mhas(label):\n\u001b[1;32m    141\u001b[0m     sumoPort \u001b[38;5;241m=\u001b[39m getFreeSocketPort() \u001b[38;5;28;01mif\u001b[39;00m port \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m port\n",
      "\u001b[0;31mTraCIException\u001b[0m: Connection 'default' is already active."
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "#Основная функция программы\n",
    "def main():\n",
    "    #Загружаем среду Starcraft II, карту, сложность противника и расширенную  \n",
    "    #награду \n",
    "    buf = ReplayBuffer(max_size=10000)\n",
    "    env = initSimulation()\n",
    "    # env = StarCraft2Env(map_name=\"3ps1zgWallFOX\", reward_only_positive=False, reward_scale_rate=200, difficulty=\"1\")\n",
    "    #Получаем и выводим на печать информацию о среде\n",
    "    # env_info = env.get_env_info()\n",
    "    # print ('env_info=',env_info)\n",
    "    #Получаем и выводим на печать размер локальных состояний среды для агента\n",
    "    # obs_size =  env_info.get('obs_shape')\n",
    "    obs_size = 18\n",
    "    print (\"obs_size=\",obs_size)\n",
    "    #Количество действий агента \n",
    "    # n_actions = env_info[\"n_actions\"]\n",
    "    n_actions = 6\n",
    "    #Количество дружественных агентов\n",
    "    n_agents = 3\n",
    "    \n",
    "    #Определяем основные параметры нейросетевого обучения    \n",
    "    ##########################################################################\n",
    "    #Некоторые переходы в алгоритме MADDPG зависят от шагов игры\n",
    "    global_step = 0 #подсчитываем общее количество шагов в игре\n",
    "    start_steps = 1000 #начинаем обучать через 1000 шагов\n",
    "    steps_train = 10 #после начала обучения продолжаем обучать каждый 4 шаг \n",
    "    #Размер минивыборки \n",
    "    batch_size = 64\n",
    "    #Общее количество эпизодов игры\n",
    "    n_episodes = 510 \n",
    "    #Параметр дисконтирования.\n",
    "    gamma = 0.99 \n",
    "    #Скорость обучения исполнителя\n",
    "    alpha_actor = 0.01\n",
    "    #Скорость обучения критика\n",
    "    alpha_critic = 0.01 \n",
    "    #Уровень случайного шума\n",
    "    noise_rate = 0.01 \n",
    "    #Начальное значение случайного шума\n",
    "    noise_rate_max = 0.9\n",
    "    #Финальное значение случайного шума\n",
    "    noise_rate_min = 0.01 \n",
    "    #Шаг затухания уровня случайного шума\n",
    "    noise_decay_steps = 15000\n",
    "    #Параметр мягкой замены\n",
    "    tau = 0.01 \n",
    "    #Объем буфера воспроизведения\n",
    "    buffer_len = 10000\n",
    "    ###########################################################################   \n",
    "        \n",
    "    #Создаем буфер воспроизведения на основе deque\n",
    "    buf = ReplayBuffer(max_size=buffer_len)\n",
    "        \n",
    "    #Pytorch определяет возможность использования графического процессора\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    #Реализуем модифицированный алгоритм MADDPG \n",
    "    #с одной нейронной сетью критика и тремя нейронными сетями исполнителей  \n",
    "    #Создаем основную нейронную сеть исполнителя\n",
    "    actor_network = MADDPG_Actor(obs_size, n_actions).to(device)\n",
    "    #Создаем целевую нейронную сеть исполнителя\n",
    "    tgtActor_network = MADDPG_Actor(obs_size, n_actions).to(device)\n",
    "    #Синхронизуем веса нейронных сетей исполнителей\n",
    "    tgtActor_network.load_state_dict(actor_network.state_dict())\n",
    "    \n",
    "    #Создаем основную нейронную сеть критика\n",
    "    critic_network = MADDPG_Critic(obs_size*n_agents, n_agents).to(device)\n",
    "    #Создаем целевую нейронную сеть критика\n",
    "    tgtCritic_network = MADDPG_Critic(obs_size*n_agents, n_agents).to(device)\n",
    "    #Синхронизуем веса нейронных сетей критиков\n",
    "    tgtCritic_network.load_state_dict(critic_network.state_dict())\n",
    "    \n",
    "    #Создаем списки для мультиагентного случая\n",
    "    actor_network_list = []\n",
    "    tgtActor_network_list = []\n",
    "    optimizerActor_list = []\n",
    "    objectiveActor_list = []\n",
    "        \n",
    "    for agent_id in range(n_agents):\n",
    "        #Создаем список основных нейронных сетей исполнителей для трех агентов\n",
    "        actor_network_list.append(actor_network)\n",
    "        #Создаем список целевых нейронных сетей исполнителей\n",
    "        tgtActor_network_list.append(tgtActor_network)\n",
    "        #Создаем список оптимизаторов нейронных сетей исполнителей\n",
    "        optimizerActor_list.append(optim.Adam(params=actor_network_list[agent_id].parameters(), lr=alpha_actor))\n",
    "        #Создаем список функций потерь исполнителей\n",
    "        objectiveActor_list.append(nn.MSELoss())\n",
    "        \n",
    "    #Создаем оптимизатор нейронной сети критика\n",
    "    optimizerCritic = optim.Adam(params=critic_network.parameters(), lr=alpha_critic)\n",
    "    #Создаем функцию потерь критика\n",
    "    objectiveCritic = nn.MSELoss()\n",
    "    \n",
    "    #Выводим на печать архитектуру нейронных сетей\n",
    "    print ('Actor_network_list=', actor_network_list)\n",
    "    print ('Critic_network_list=', critic_network)\n",
    "            \n",
    "    #Определяем вспомогательные параметры\n",
    "    Loss_History = [] \n",
    "    Loss_History_actor = []\n",
    "    Reward_History = []\n",
    "    winrate_history = []\n",
    "    total_loss = []\n",
    "    total_loss_actor = []\n",
    "    m_loss = []\n",
    "    m_loss_actor = []\n",
    "    \n",
    "\n",
    "    #Основной цикл по эпизодам игры\n",
    "    ################_цикл for по эпизодам_#####################################\n",
    "    for e in range(n_episodes):\n",
    "       \n",
    "        #Перезагружаем среду\n",
    "        env.close()\n",
    "        env = initSimulation()\n",
    "        #Флаг окончания эпизода\n",
    "        terminated = False\n",
    "        #Награда за эпизод\n",
    "        episode_reward = 0\n",
    "        #Обновляем и выводим динамический уровень случайного шума\n",
    "        noise_rate = max(noise_rate_min, noise_rate_max - (noise_rate_max-noise_rate_min) * global_step/noise_decay_steps)\n",
    "        print ('noise_rate=', noise_rate)\n",
    "                \n",
    "        #Шаги игры внутри эпизода\n",
    "        ######################_цикл while_#####################################\n",
    "        while not terminated:\n",
    "            #Обнуляем промежуточные переменные\n",
    "            actions = []\n",
    "            observations = []\n",
    "            action = 0\n",
    "            #Храним историю действий один шаг для разных агентов\n",
    "            actionsFox = np.zeros([n_agents]) \n",
    "            #Храним историю состояний среды один шаг для разных агентов\n",
    "            obs_agent = np.zeros([n_agents], dtype=object) \n",
    "            obs_agent_next = np.zeros([n_agents], dtype=object)\n",
    "            \n",
    "            done = False\n",
    "\n",
    "            ###########_Цикл по агентам для выполнения действий в игре_########\n",
    "            for agent_id in range(n_agents):\n",
    "                #Получаем состояние среды для независимого агента \n",
    "                obs_agent[agent_id] = env.get_obs_agent(agent_id)\n",
    "                # if tuple(obs_agent[agent_id]) == tuple(DEFAULT_STATE):\n",
    "                #     done = True\n",
    "\n",
    "                #Конвертируем данные в тензор\n",
    "                obs_agentT = torch.FloatTensor([obs_agent[agent_id]]).to(device)\n",
    "                #Передаем состояние среды в основную нейронную сеть \n",
    "                #и получаем стратегию действий\n",
    "                action_probabilitiesT = actor_network_list[agent_id](obs_agentT)\n",
    "                #Конвертируем данные в numpy\n",
    "                action_probabilitiesT = action_probabilitiesT.to(\"cpu\")\n",
    "                action_probabilities = action_probabilitiesT.data.numpy()[0]\n",
    "                \n",
    "                #Находим возможные действия агента в данный момент времени \n",
    "                avail_actions = env.get_avail_agent_actions(agent_id)\n",
    "                avail_actions_ind = np.nonzero(avail_actions)[0]\n",
    "                #Выбираем возможное действие агента с учетом\n",
    "                #стратегии действий и уровня случайного шума\n",
    "                action = select_actionFox(action_probabilities, avail_actions_ind, n_actions, noise_rate)\n",
    "                #Обрабатываем исключение при ошибке в возможных действиях\n",
    "                if action is None:\n",
    "                    action = np.random.choice (avail_actions_ind)\n",
    "                    \n",
    "                #Собираем действия от разных агентов               \n",
    "                actions.append(action)\n",
    "                actionsFox[agent_id] = action\n",
    "                #Собираем локальные состояния среды от разных агентов\n",
    "                for i in range(obs_size):\n",
    "                    observations.append(obs_agent[agent_id][i])\n",
    "            ######_конец цикла по агентам для выполнения действий в игре_######\n",
    "\n",
    "            #Передаем действия агентов в среду, получаем награду\n",
    "            #и прерывание игры от среды\n",
    "            reward, terminated = env.step(actions)\n",
    "            #Суммируем награды за этот шаг для вычисления награды за эпизод\n",
    "            episode_reward += reward\n",
    "            \n",
    "            #Подготовляем данные для сохранения в буфере воспроизведения\n",
    "            actions_next = []\n",
    "            observations_next = []\n",
    "            #Если эпизод не завершился, то можно найти новые действия и состояния\n",
    "            if terminated == False:\n",
    "                for agent_id in range(n_agents):\n",
    "                    #Получаем новое состояние среды для независимого агента \n",
    "                    obs_agent_next[agent_id] = env.get_obs_agent(agent_id)\n",
    "\n",
    "                    # if tuple(obs_agent_next[agent_id]) == tuple(DEFAULT_STATE):\n",
    "                    #     done = True\n",
    "                    #Собираем от разных агентов новые состояния\n",
    "                    for i in range(obs_size):\n",
    "                        observations_next.append(obs_agent_next[agent_id][i])\n",
    "                    #Конвертируем данные в тензор\n",
    "                    obs_agent_nextT = torch.FloatTensor([obs_agent_next[agent_id]]).to(device)\n",
    "                    #Получаем новые действия агентов для новых состояний\n",
    "                    #из целевой сети исполнителя\n",
    "                    action_probabilitiesT = tgtActor_network_list[agent_id](obs_agent_nextT)\n",
    "                    #Конвертируем данные в numpy\n",
    "                    action_probabilitiesT = action_probabilitiesT.to(\"cpu\")\n",
    "                    action_probabilities = action_probabilitiesT.data.numpy()[0]\n",
    "                    #Находим новые возможные действия агента\n",
    "                    avail_actions = env.get_avail_agent_actions(agent_id)\n",
    "                    avail_actions_ind = np.nonzero(avail_actions)[0]\n",
    "                    #Выбираем новые возможные действия\n",
    "                    action = select_actionFox(action_probabilities, avail_actions_ind, n_actions, noise_rate)\n",
    "                    if action == 0:\n",
    "                        pass\n",
    "                    if action is None:\n",
    "                        action = np.random.choice (avail_actions_ind)\n",
    "                    #Собираем новые действия от разных агентов\n",
    "\n",
    "                    actions_next.append(action)\n",
    "            elif terminated == True:\n",
    "                #если эпизод на этом шаге завершился, то новых действий не будет\n",
    "                actions_next = actions \n",
    "\n",
    "                observations_next = observations\n",
    "            \n",
    "            #Сохраняем переход в буфере воспроизведения \n",
    "            if not done:\n",
    "                pass\n",
    "            buf.put((observations, actions, observations_next, actions_next, reward, terminated or done))\n",
    "            \n",
    "            #Если буфер воспроизведения наполнен, начинаем обучать сеть\n",
    "            ########################_начало if обучения_#######################\n",
    "            if (global_step % steps_train == 0) and (global_step > start_steps):\n",
    "                #Получаем минивыборку из буфера воспроизведения\n",
    "                exp_obs, exp_acts, exp_next_obs, exp_next_acts, exp_rew, exp_termd = buf.sample(batch_size)\n",
    "                    \n",
    "                #Конвертируем данные в тензор\n",
    "                exp_obs = [np.array(x) for x in exp_obs]\n",
    "                obs_agentsT = torch.FloatTensor([exp_obs]).to(device)\n",
    "                exp_acts = [np.array(x) for x in exp_acts]\n",
    "                act_agentsT = torch.FloatTensor([exp_acts]).to(device)\n",
    "                                    \n",
    "                ###############_Обучаем нейронную сеть критика_################\n",
    "                \n",
    "                #Получаем значения из основной сети критика\n",
    "                action_probabilitieQT = critic_network(obs_agentsT, act_agentsT)\n",
    "                action_probabilitieQT = action_probabilitieQT.to(\"cpu\")\n",
    "                               \n",
    "                #Конвертируем данные в тензор\n",
    "                exp_next_obs = [np.array(x) for x in exp_next_obs]\n",
    "                obs_agents_nextT = torch.FloatTensor([exp_next_obs]).to(device)\n",
    "                exp_next_acts = [np.array(x) for x in exp_next_acts]\n",
    "                act_agents_nextT = torch.FloatTensor([exp_next_acts]).to(device)\n",
    "                                        \n",
    "                #Получаем значения из целевой сети критика\n",
    "                action_probabilitieQ_nextT = tgtCritic_network(obs_agents_nextT, act_agents_nextT)\n",
    "                action_probabilitieQ_nextT = action_probabilitieQ_nextT.to(\"cpu\")\n",
    "                action_probabilitieQ_next = action_probabilitieQ_nextT.data.numpy()[0]\n",
    "                    \n",
    "                #Переформатируем y_batch размером batch_size\n",
    "                y_batch = np.zeros([batch_size])\n",
    "                action_probabilitieQBT = torch.empty(1, batch_size, dtype=torch.float)\n",
    "                \n",
    "                for i in range (batch_size):\n",
    "                    #Вычисляем целевое значение y \n",
    "                    y_batch[i] = exp_rew[i] + (gamma*action_probabilitieQ_next[i])*(1 - np.array(exp_termd[i]))\n",
    "                    action_probabilitieQBT[0][i] = action_probabilitieQT[0][i]\n",
    "                \n",
    "                y_batchT = torch.FloatTensor([y_batch])\n",
    "                \n",
    "                #Обнуляем градиенты\n",
    "                optimizerCritic.zero_grad()\n",
    "                 \n",
    "                #Вычисляем функцию потерь критика\n",
    "                loss_t_critic = objectiveCritic(action_probabilitieQBT, y_batchT) \n",
    "                    \n",
    "                #Сохраняем данные для графиков\n",
    "                Loss_History.append(loss_t_critic) \n",
    "                loss_n_critic = loss_t_critic.data.numpy()\n",
    "                total_loss.append(loss_n_critic)\n",
    "                m_loss.append(np.mean(total_loss[-1000:]))\n",
    "                    \n",
    "                #Выполняем обратное распространение ошибки для критика\n",
    "                loss_t_critic.backward()\n",
    "                \n",
    "                #Выполняем оптимизацию нейронной сети критика\n",
    "                optimizerCritic.step()\n",
    "                ###################_Закончили обучать критика_#################\n",
    "                \n",
    "                ##############_Обучаем нейронные сети исполнителей_############\n",
    "                #Разбираем совместное состояние на локальные состояния\n",
    "\n",
    "                act_full = np.zeros([batch_size, n_agents])\n",
    "\n",
    "                for z in range(n_agents):\n",
    "                    obs_localz = np.zeros([batch_size, obs_size])\n",
    "                    for i in range (batch_size):\n",
    "                        k = 0\n",
    "                        for j in range (obs_size*z, obs_size*(z + 1)):\n",
    "                            obs_localz[i][k] = exp_obs[i][j]\n",
    "                            k += 1\n",
    "\n",
    "                    obs_agentTz = torch.FloatTensor([obs_localz]).to(device)\n",
    "                    optimizerActor_list[z].zero_grad()\n",
    "                    action_probabilitiesT1 = actor_network_list[z](obs_agentTz)\n",
    "                    action_probabilitiesT1 = action_probabilitiesT1.to(\"cpu\")\n",
    "                    action_probabilities1 = action_probabilitiesT1.data.numpy()[0]\n",
    "\n",
    "                    for i in range (batch_size):\n",
    "                        act_full[i][z] = np.argmax(action_probabilities1[i])\n",
    "                        # act_full[i][1] = np.argmax(action_probabilities2[i])\n",
    "                        # act_full[i][2] = np.argmax(action_probabilities3[i])\n",
    "\n",
    "                act_fullT = torch.FloatTensor([act_full]).to(device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # obs_local1 = np.zeros([batch_size, obs_size])\n",
    "                # obs_local2 = np.zeros([batch_size, obs_size])\n",
    "                # obs_local3 = np.zeros([batch_size, obs_size])\n",
    "                # for i in range (batch_size):\n",
    "                #     for j in range (obs_size):\n",
    "                #          obs_local1[i][j] = exp_obs[i][j]\n",
    "                # for i in range (batch_size):\n",
    "                #     k=0\n",
    "                #     for j in range (obs_size, obs_size*2):\n",
    "                #          obs_local2[i][k] = exp_obs[i][j]\n",
    "                #          k = k + 1\n",
    "                # for i in range (batch_size):\n",
    "                #     k=0\n",
    "                #     for j in range (obs_size*2, obs_size*3):\n",
    "                #          obs_local3[i][k] = exp_obs[i][j]\n",
    "                #          k = k + 1\n",
    "                # #Конвертируем данные в тензор                \n",
    "                # obs_agentT1 = torch.FloatTensor([obs_local1]).to(device)\n",
    "                # obs_agentT2 = torch.FloatTensor([obs_local2]).to(device)\n",
    "                # obs_agentT3 = torch.FloatTensor([obs_local3]).to(device)\n",
    "                \n",
    "                # #Обнуляем градиенты \n",
    "                # optimizerActor_list[0].zero_grad()\n",
    "                # optimizerActor_list[1].zero_grad()\n",
    "                # optimizerActor_list[2].zero_grad()\n",
    "                \n",
    "                # #Подаем в нейронные сети исполнителей локальные состояния\n",
    "                # action_probabilitiesT1 = actor_network_list[0](obs_agentT1)\n",
    "                # action_probabilitiesT2 = actor_network_list[1](obs_agentT2)\n",
    "                # action_probabilitiesT3 = actor_network_list[2](obs_agentT3)\n",
    "                                \n",
    "                # #Конвертируем данные в numpy\n",
    "                # action_probabilitiesT1 = action_probabilitiesT1.to(\"cpu\")\n",
    "                # action_probabilitiesT2 = action_probabilitiesT2.to(\"cpu\")\n",
    "                # action_probabilitiesT3 = action_probabilitiesT3.to(\"cpu\")\n",
    "                # action_probabilities1 = action_probabilitiesT1.data.numpy()[0]\n",
    "                # action_probabilities2 = action_probabilitiesT2.data.numpy()[0]\n",
    "                # action_probabilities3 = action_probabilitiesT3.data.numpy()[0]\n",
    "                \n",
    "                # #Вычисляем максимальные значения с учетом объема минивыборки\n",
    "                # act_full = np.zeros([batch_size, n_agents])\n",
    "                # for i in range (batch_size):\n",
    "                #     act_full[i][0] = np.argmax(action_probabilities1[i])\n",
    "                #     act_full[i][1] = np.argmax(action_probabilities2[i])\n",
    "                #     act_full[i][2] = np.argmax(action_probabilities3[i])\n",
    "                # act_fullT = torch.FloatTensor([act_full]).to(device)\n",
    "                \n",
    "                #Конвертируем данные в тензор\n",
    "                exp_obs = [x for x in exp_obs]\n",
    "                obs_agentsT = torch.FloatTensor([exp_obs]).to(device)\n",
    "                                \n",
    "                #Задаем значение функции потерь для нерйонных сетей исполнителей\n",
    "                #как отрицательный выход критика\n",
    "                actor_lossT = -critic_network(obs_agentsT, act_fullT)\n",
    "                \n",
    "                #Усредняем значение по количеству элементов минивыборки\n",
    "                actor_lossT = actor_lossT.mean()    \n",
    "                \n",
    "                #Выполняем обратное распространение ошибки\n",
    "                actor_lossT.backward()\n",
    "                \n",
    "                #Выполняем оптимизацию нейронных сетей исполнителей\n",
    "                for z in range(n_agents):\n",
    "                    optimizerActor_list[z].step()\n",
    "                    # optimizerActor_list[1].step()\n",
    "                    # optimizerActor_list[2].step()\n",
    "                \n",
    "                #Собираем данные для графиков\n",
    "                actor_lossT = actor_lossT.to(\"cpu\")\n",
    "                Loss_History_actor.append(actor_lossT) \n",
    "                actor_lossN = actor_lossT.data.numpy()\n",
    "                total_loss_actor.append(actor_lossN)\n",
    "                m_loss_actor.append(np.mean(total_loss_actor[-1000:]))\n",
    "    \n",
    "                ##############_Закончили обучать исполнителей_#################\n",
    "                \n",
    "                #Рализуем механизм мягкой замены\n",
    "                #Обновляем целевую сеть критика\n",
    "                for target_param, param in zip(tgtCritic_network.parameters(), critic_network.parameters()):\n",
    "                    target_param.data.copy_((1 - tau) * param.data + tau * target_param.data)\n",
    "                #Обновляем целевые сети акторов\n",
    "                for agent_id in range(n_agents):\n",
    "                    for target_param, param in zip(tgtActor_network_list[agent_id].parameters(), actor_network_list[agent_id].parameters()):\n",
    "                        target_param.data.copy_((1 - tau) * param.data + tau * target_param.data)\n",
    " \n",
    "                ######################_конец if обучения_######################\n",
    "                \n",
    "            #Обновляем счетчик общего количества шагов\n",
    "            global_step += 1\n",
    "        \n",
    "        ######################_конец цикла while_##############################\n",
    "        writer.add_scalar('reward/train', episode_reward , e)\n",
    "        #Выводим на печать счетчик шагов игры и общую награду за эпизод\n",
    "        print('global_step=', global_step, \"Total reward in episode {} = {}\".format(e, episode_reward))\n",
    "        print(\"loss1\", total_loss[-10:])\n",
    "        print(\"loss2\", total_loss_actor[-10:])\n",
    "        \n",
    "        #Собираем данные для графиков\n",
    "        Reward_History.append(episode_reward)\n",
    "        # status = env.get_stats()\n",
    "        # winrate_history.append(status[\"win_rate\"])\n",
    "        \n",
    "    ################_конец цикла по эпизодам игры_#############################\n",
    "    \n",
    "    #Закрываем среду StarCraft II\n",
    "    env.close()\n",
    "    \n",
    "    #Сохраняем параметры обученных нейронных сетей\n",
    "    for agent_id in range(n_agents):\n",
    "        torch.save(actor_network_list[agent_id].state_dict(),\"actornet_%.0f.dat\"%agent_id) \n",
    "    \n",
    "    #Выводим на печать графики\n",
    "    #Средняя награда\n",
    "    plt.figure(num=None, figsize=(6, 3), dpi=150, facecolor='w', edgecolor='k')\n",
    "    plt.plot(Reward_History)\n",
    "    plt.xlabel('Номер эпизода')\n",
    "    plt.ylabel('Количество награды за эпизод')\n",
    "    plt.show()\n",
    "    #Процент побед\n",
    "    plt.figure(num=None, figsize=(6, 3), dpi=150, facecolor='w', edgecolor='k')\n",
    "    plt.plot(winrate_history)\n",
    "    plt.xlabel('Номер эпизода')\n",
    "    plt.ylabel('Процент побед')\n",
    "    plt.show()\n",
    "    #Значения функции потерь исполнителя\n",
    "    plt.figure(num=None, figsize=(6, 3), dpi=150, facecolor='w', edgecolor='k')\n",
    "    plt.plot(m_loss_actor)\n",
    "    plt.xlabel('Номер каждой 1000 итерации')\n",
    "    plt.ylabel('Функция потерь исполнителя')\n",
    "    plt.show()\n",
    "    #Значения функции потерь критика\n",
    "    plt.figure(num=None, figsize=(6, 3), dpi=150, facecolor='w', edgecolor='k')\n",
    "    plt.plot(m_loss)\n",
    "    plt.xlabel('Номер каждой 1000 итерации')\n",
    "    plt.ylabel('Функция потерь критика')\n",
    "    plt.show()\n",
    "   \n",
    "#Точка входа в программу  \n",
    "if __name__ == \"__main__\":\n",
    "    start_time = time.time()\n",
    "    main() \n",
    "    print(\"--- %s минут ---\" % ((time.time() - start_time)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43menv\u001b[49m\u001b[38;5;241m.\u001b[39mclose()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'env' is not defined"
     ]
    }
   ],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'chess'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpettingzoo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclassic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m chess_v6\n\u001b[1;32m      3\u001b[0m env \u001b[38;5;241m=\u001b[39m chess_v6\u001b[38;5;241m.\u001b[39menv(render_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m env\u001b[38;5;241m.\u001b[39mreset(seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pettingzoo/classic/__init__.py:5\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(env_name)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(env_name):\n\u001b[0;32m----> 5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdeprecated_handler\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m__path__\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pettingzoo/utils/deprecated_module.py:60\u001b[0m, in \u001b[0;36mdeprecated_handler\u001b[0;34m(env_name, module_path, module_name)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# This executes the module and will raise any exceptions\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# that would typically be raised by just `import blah`\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m spec\u001b[38;5;241m.\u001b[39mloader\n\u001b[0;32m---> 60\u001b[0m \u001b[43mspec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexec_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pettingzoo/classic/chess_v6.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpettingzoo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclassic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchess\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchess\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m env, raw_env\n\u001b[1;32m      3\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menv\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_env\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pettingzoo/classic/chess/chess.py:109\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m__future__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m annotations\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m path\n\u001b[0;32m--> 109\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mchess\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgymnasium\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'chess'"
     ]
    }
   ],
   "source": [
    "from pettingzoo.classic import chess_v6\n",
    "\n",
    "env = chess_v6.env(render_mode=\"human\")\n",
    "env.reset(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43menv\u001b[49m\u001b[38;5;241m.\u001b[39maction_spaces\n",
      "\u001b[0;31mNameError\u001b[0m: name 'env' is not defined"
     ]
    }
   ],
   "source": [
    "env.action_spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting chess\n",
      "  Downloading chess-1.10.0-py3-none-any.whl (154 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.4/154.4 KB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: chess\n",
      "Successfully installed chess-1.10.0\n"
     ]
    }
   ],
   "source": [
    "!pip install chess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method BaseWrapper.action_space of <pettingzoo.utils.wrappers.assert_out_of_bounds.AssertOutOfBoundsWrapper object at 0x7fe19c12ff40>>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
